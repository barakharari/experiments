{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d196a1",
   "metadata": {},
   "source": [
    "### Sensitivity Experiments\n",
    "#### Methodologies for sensitvity assessment\n",
    "1) LLM based zero-shot/few-shot (\"Categorize sensitivity into these 'x' categories\")\n",
    "2) Transformer based PII extraction. Train sensitivity scorer on top of results (map score to category)\n",
    "3) TF-IDF for topics. Train sensitivity scorer on top of results (map score to category)\n",
    "#### Explainability\n",
    "1) LIME/SHAP to identify most influential features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into pandas DataFrames\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_dir = \"./csvs\"\n",
    "\n",
    "dfs = {}\n",
    "for filename in os.listdir(csv_dir):\n",
    "    path = csv_dir + \"/\" + filename\n",
    "    try:\n",
    "        name_no_ext = filename.split('.')[0]\n",
    "        dfs[name_no_ext] = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d393d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Print brief summary and show first few rows for each loaded dataframe\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24decebc",
   "metadata": {},
   "source": [
    "# Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe27590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LLM input\n",
    "\n",
    "llm_inputs = {}\n",
    "for _, label in dfs['resource_label'].iterrows():\n",
    "  label_type = label['name']\n",
    "  label_value = label['value']\n",
    "\n",
    "  input = llm_inputs.setdefault(label['hash_id'], {})\n",
    "  label_type_values = input.setdefault(label_type + \"s\", [])\n",
    "  label_type_values.append(label_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6508d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Remove inputs that don't have much information (likely indexing went wrong)\n",
    "valid_llm_inputs = {k: v for k, v in llm_inputs.items() if len(v.keys()) >= 2}\n",
    "\n",
    "NUM_INPUTS=30\n",
    "keys = list(valid_llm_inputs.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "keys = keys[:NUM_INPUTS]\n",
    "reduced_inputs = {k: v for k, v in llm_inputs.items() if k in keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cf980",
   "metadata": {},
   "source": [
    "# LLM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cfe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM query\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# Load config\n",
    "openai_config_path = \"./configs/openai_standard.json\"\n",
    "with open(openai_config_path, \"r\") as f:\n",
    "  openai_config = json.loads(f.read())\n",
    "\n",
    "# LLM client\n",
    "client = openai.AzureOpenAI(\n",
    "  azure_endpoint=openai_config['endpoint'],\n",
    "  api_version=openai_config['api_version'],\n",
    "  api_key=openai_config['key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert at assessing document sensitivity levels. You will receive document metadata in JSON format and must categorize each document's sensitivity.\n",
    "\n",
    "INPUT FORMAT:\n",
    "{\n",
    "  \"<document_id>\": {\n",
    "    \"document_types\": [\"<document_type_1>\", \"<document_type_n>\"],\n",
    "    \"primary_subjects\": [\"<primary_subject_1>\", \"<primary_subject_n>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "Note: Input may include additional fields such as \"summary\", \"names\", etc.\n",
    "\n",
    "SENSITIVITY CATEGORIES:\n",
    "Categorize each document into one of the following levels:\n",
    "- Public: Information suitable for public disclosure\n",
    "- Internal: Information for internal use only\n",
    "- Confidential: Sensitive business information with limited access\n",
    "- Restricted: Highly sensitive information (e.g., PII, financial data, trade secrets)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return valid JSON that can be parsed by Python's json.loads():\n",
    "{\n",
    "  \"document_id\": \"<document_id>\",\n",
    "  \"category\": \"<sensitivity_category>\",\n",
    "  \"confidence\": \"<confidence_percentage>\",\n",
    "  \"explanation\": \"<brief_justification>\"\n",
    "}\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{\n",
    "  \"document_id\": \"142\",\n",
    "  \"category\": \"Restricted\",\n",
    "  \"confidence\": \"80%\",\n",
    "  \"explanation\": \"Contains PII/PCI including SSNs and bank account information\"\n",
    "}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Output must be valid JSON parseable by json.loads()\n",
    "- Use consistent capitalization for category names (e.g., \"Restricted\" not \"RESTRICTED\")\n",
    "- Include all four fields: document_id, category, confidence, explanation\n",
    "- Provide clear, concise explanations for your categorization\"\"\"\n",
    "\n",
    "USER_PROMPT = f\"\"\"\n",
    "====START DOCUMENT INFORMATION====\n",
    "{reduced_inputs}\n",
    "====END DOCUMENT INFORMATION====\n",
    "\"\"\"\n",
    "\n",
    "print(USER_PROMPT)\n",
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"developer\",\n",
    "      \"content\": SYSTEM_PROMPT\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": USER_PROMPT\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9753ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = completion.choices[0].message.content\n",
    "ans = ans[ans.find('['):ans.rfind(']')+1]\n",
    "json.loads(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LLM JSON output to CSV for Google Sheets\n",
    "# Assumes the LLM JSON contains the four fields: document_id, category, confidence, explanation\n",
    "# Expands the original input so each member of any array becomes its own column\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure we have the LLM answer (ans) available\n",
    "if 'ans' not in globals():\n",
    "    if 'completion' in globals():\n",
    "        ans = completion.choices[0].message.content\n",
    "        if '[' in ans and ']' in ans:\n",
    "            ans = ans[ans.find('['):ans.rfind(']')+1]\n",
    "    else:\n",
    "        raise NameError(\"Neither 'ans' nor 'completion' found. Run the LLM completion cell first.\")\n",
    "\n",
    "# Parse JSON directly (we assume valid JSON with the four expected fields)\n",
    "results = json.loads(ans)\n",
    "\n",
    "# Normalize results into a list of records\n",
    "if isinstance(results, list):\n",
    "    records = results\n",
    "elif isinstance(results, dict):\n",
    "    # If dict maps document_id -> {fields}, convert to list\n",
    "    if all(isinstance(v, dict) for v in results.values()):\n",
    "        records = []\n",
    "        for k, v in results.items():\n",
    "            row = dict(v)\n",
    "            if 'document_id' not in row:\n",
    "                row['document_id'] = k\n",
    "            records.append(row)\n",
    "    else:\n",
    "        # Single-record dict with the four fields\n",
    "        records = [results]\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure: {type(results)}\")\n",
    "\n",
    "# Build DataFrame (expects columns: document_id, category, confidence, explanation)\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Attach and expand the original input for each document if available (prefer reduced_inputs)\n",
    "input_map = globals().get('reduced_inputs') or globals().get('llm_inputs') or {}\n",
    "\n",
    "# Build expanded input rows\n",
    "input_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    doc_id = row.get('document_id')\n",
    "    # try keys in multiple forms\n",
    "    found = None\n",
    "    for k in (doc_id, str(doc_id)):\n",
    "        if k in input_map:\n",
    "            found = input_map[k]\n",
    "            break\n",
    "    if found is None:\n",
    "        # try loose match by string equality\n",
    "        for k in input_map.keys():\n",
    "            if str(k) == str(doc_id):\n",
    "                found = input_map[k]\n",
    "                break\n",
    "    if found is None:\n",
    "        found = {}\n",
    "\n",
    "    expanded = {'document_id': doc_id}\n",
    "    # For each key in the input dict, expand lists into numbered columns,\n",
    "    # keep scalars as-is (strings/numbers)\n",
    "    for key, val in found.items():\n",
    "        if isinstance(val, (list, tuple)):\n",
    "            for i, v in enumerate(val):\n",
    "                expanded[f\"{key}_{i}\"] = v\n",
    "        else:\n",
    "            expanded[key] = val\n",
    "\n",
    "    input_rows.append(expanded)\n",
    "\n",
    "if input_rows:\n",
    "    inputs_df = pd.DataFrame(input_rows)\n",
    "    # Ensure both document_id are strings for a reliable merge\n",
    "    df['document_id'] = df['document_id'].astype(str)\n",
    "    inputs_df['document_id'] = inputs_df['document_id'].astype(str)\n",
    "    merged = df.merge(inputs_df, on='document_id', how='left')\n",
    "else:\n",
    "    # No inputs to expand; attach an empty placeholder\n",
    "    merged = df.copy()\n",
    "\n",
    "# Save CSV ready for Google Sheets upload\n",
    "out_path = Path(\"sensitivity_results_\" + str(datetime.now()) + \".csv\")\n",
    "merged.to_csv(out_path, index=False)\n",
    "print(f\"Saved CSV to: {out_path.resolve()} (shape: {merged.shape})\")\n",
    "\n",
    "# Display a preview\n",
    "display(merged.head())\n",
    "\n",
    "# Expose variables for interactive use\n",
    "sensitivity_results_df = merged\n",
    "sensitivity_results_csv = out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598e0e5",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576808a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_string(inp):\n",
    "  str = \"| \"\n",
    "  for category, values in inp.items():\n",
    "    str += category + \": \"\n",
    "    for value in values[:-1]:\n",
    "      str += value + \", \"\n",
    "    str += values[-1]\n",
    "    str += \" | \"\n",
    "  return str\n",
    "\n",
    "INDEX = 29\n",
    "text = f\"\"\"\n",
    "{json_to_string(list(reduced_inputs.values())[INDEX])}\n",
    "\"\"\"\n",
    "\n",
    "print(list(reduced_inputs.keys())[INDEX])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_multi_pii-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2541a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"booking number\", \"personally identifiable information\", \"driver licence\", \"person\", \"book\", \"full address\", \"company\", \"actor\", \"character\", \"email\", \"passport number\", \"Social Security Number\", \"phone number\", \"financial data\"]\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity)\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers==4.50.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e17b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105389e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "# Get the model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9039b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "# Convert token predictions to word predictions\n",
    "encoded_inputs = tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "offset_mapping = encoded_inputs['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a19ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_pii(text, aggregate_redaction=True):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get the model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert token predictions to word predictions\n",
    "    encoded_inputs = tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "    offset_mapping = encoded_inputs['offset_mapping']\n",
    "\n",
    "    masked_text = list(text)\n",
    "    is_redacting = False\n",
    "    redaction_start = 0\n",
    "    current_pii_type = ''\n",
    "\n",
    "    for i, (start, end) in enumerate(offset_mapping):\n",
    "        if start == end:  # Special token\n",
    "            continue\n",
    "\n",
    "        label = predictions[0][i].item()\n",
    "        if label != model.config.label2id['O']:  # Non-O label\n",
    "            pii_type = model.config.id2label[label]\n",
    "            if not is_redacting:\n",
    "                is_redacting = True\n",
    "                redaction_start = start\n",
    "                current_pii_type = pii_type\n",
    "            elif not aggregate_redaction and pii_type != current_pii_type:\n",
    "                # End current redaction and start a new one\n",
    "                apply_redaction(masked_text, redaction_start, start, current_pii_type, aggregate_redaction)\n",
    "                redaction_start = start\n",
    "                current_pii_type = pii_type\n",
    "        else:\n",
    "            if is_redacting:\n",
    "                apply_redaction(masked_text, redaction_start, end, current_pii_type, aggregate_redaction)\n",
    "                is_redacting = False\n",
    "\n",
    "    # Handle case where PII is at the end of the text\n",
    "    if is_redacting:\n",
    "        apply_redaction(masked_text, redaction_start, len(masked_text), current_pii_type, aggregate_redaction)\n",
    "\n",
    "    return ''.join(masked_text)\n",
    "\n",
    "def apply_redaction(masked_text, start, end, pii_type, aggregate_redaction):\n",
    "    for j in range(start, end):\n",
    "        masked_text[j] = ''\n",
    "    if aggregate_redaction:\n",
    "        masked_text[start] = '[redacted]'\n",
    "    else:\n",
    "        masked_text[start] = f'[{pii_type}]'\n",
    "\n",
    "print(\"Aggregated redaction:\")\n",
    "masked_example_aggregated = mask_pii(text, aggregate_redaction=True)\n",
    "print(masked_example_aggregated)\n",
    "\n",
    "print(\"\\nDetailed redaction:\")\n",
    "masked_example_detailed = mask_pii(text, aggregate_redaction=False)\n",
    "print(masked_example_detailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4061564",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"names\", \"personally identifiable information\", \"financial information\", \"financial figures\", \"revenue\"]\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d393d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Print brief summary and show first few rows for each loaded dataframe\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert at assessing document sensitivity levels. You will receive document metadata in JSON format and must categorize each document's sensitivity.\n",
    "\n",
    "INPUT FORMAT:\n",
    "{\n",
    "  \"<document_id>\": {\n",
    "    \"document_types\": [\"<document_type_1>\", \"<document_type_n>\"],\n",
    "    \"primary_subjects\": [\"<primary_subject_1>\", \"<primary_subject_n>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "Note: Input may include additional fields such as \"summary\", \"names\", etc.\n",
    "\n",
    "SENSITIVITY CATEGORIES:\n",
    "Categorize each document into one of the following levels:\n",
    "- Public: Information suitable for public disclosure\n",
    "- Internal: Information for internal use only\n",
    "- Confidential: Sensitive business information with limited access\n",
    "- Restricted: Highly sensitive information (e.g., PII, financial data, trade secrets)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return valid JSON that can be parsed by Python's json.loads():\n",
    "{\n",
    "  \"document_id\": \"<document_id>\",\n",
    "  \"category\": \"<sensitivity_category>\",\n",
    "  \"confidence\": \"<confidence_percentage>\",\n",
    "  \"explanation\": \"<brief_justification>\"\n",
    "}\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{\n",
    "  \"document_id\": \"142\",\n",
    "  \"category\": \"Restricted\",\n",
    "  \"confidence\": \"80%\",\n",
    "  \"explanation\": \"Contains PII/PCI including SSNs and bank account information\"\n",
    "}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Output must be valid JSON parseable by json.loads()\n",
    "- Use consistent capitalization for category names (e.g., \"Restricted\" not \"RESTRICTED\")\n",
    "- Include all four fields: document_id, category, confidence, explanation\n",
    "- Provide clear, concise explanations for your categorization\"\"\"\n",
    "\n",
    "USER_PROMPT = f\"\"\"\n",
    "====START DOCUMENT INFORMATION====\n",
    "{reduced_inputs}\n",
    "====END DOCUMENT INFORMATION====\n",
    "\"\"\"\n",
    "\n",
    "print(USER_PROMPT)\n",
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9753ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = completion.choices[0].message.content\n",
    "ans = ans[ans.find('['):ans.rfind(']')+1]\n",
    "json.loads(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf42e87",
   "metadata": {},
   "source": [
    "# Contextualizing the data- connecting to audit log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8faeb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "rar = dfs['resource_auditrecord']\n",
    "rar[rar['audited_id'] == 2443586]['operation'].unique()\n",
    "rar[rar['audited_id'] == 2443586][rar['operation'] == \"MODIFIED\"].sort_values(by=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ae3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "rrn = dfs['resource_resourcenode']\n",
    "rrn[rrn['id'] == 2443586]\n",
    "# rrn[rrn['resource_id'] == 813557]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through resource_label_assignees\n",
    "\n",
    "dfs[\"resource_label\"][\"name\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7637189",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr = dfs['resource_resource']\n",
    "rr[rr['parent_id'] == 813557].sort_values(by=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = dfs['resource_hash']\n",
    "rh[rh['id'] == 81531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_times = rar[rar['audited_id'] == 2443586]['timestamp']\n",
    "\n",
    "rr = dfs['resource_resource']\n",
    "rr = rr[rr['id'] == 813557]\n",
    "\n",
    "# display(rr)\n",
    "# target_time = \"2025-10-08 10:21:52.542+00\"\n",
    "# target_time = datetime.fromisoformat(target_time)\n",
    "timestamps = list(rr['timestamp'])\n",
    "\n",
    "for audit_time in audit_times:\n",
    "  if (audit_time in timestamps):\n",
    "    display(rar[rar['audited_id'] == 2443586][rar['timestamp'] == audit_time])\n",
    "    print(\"FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['resource_label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104d78c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting all of the streams topics/access metadata under one parent resource\n",
    "# 1) Iterate through audit log\n",
    "\n",
    "'''\n",
    "\"resource_id\": {\n",
    "  metadata...\n",
    "  history: [\n",
    "    operation\n",
    "    time_of_access\n",
    "    ip\n",
    "    resource_information\n",
    "  ]\n",
    "}\n",
    "'''\n",
    "resources = {}\n",
    "\n",
    "COUNT = 200\n",
    "\n",
    "for ar in dfs['resource_auditrecord'].iterrows():\n",
    "  print(\"Processing entry\")\n",
    "  ar = ar[1]\n",
    "  timestamp, operation, location, ip, user_id, rn_id = ar['timestamp'], ar['operation'], ar['geolocation'], ar['client_ip'], ar['user_id'], ar['audited_id']\n",
    "  resource_info = resources.setdefault(rn_id, {})\n",
    "  resource_history = resource_info.setdefault(\"history\", [])\n",
    "\n",
    "  # Info from audit record\n",
    "  event = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"operation\": operation,\n",
    "    \"location\": location,\n",
    "    \"ip\": ip,\n",
    "    \"user_id\": user_id,\n",
    "    \"labels\": {}\n",
    "  }\n",
    "\n",
    "  # Info from resource node\n",
    "  rrn = dfs['resource_resourcenode']\n",
    "  resource_node = rrn[rrn['id'] == rn_id]\n",
    "  resource_id = resource_node['resource_id']\n",
    "\n",
    "  assert len(resource_id) == 1\n",
    "\n",
    "  resource_id = resource_id.item()\n",
    "\n",
    "  # Info from resource\n",
    "  rr, rl = dfs['resource_resource'], dfs['resource_label']\n",
    "  resource_entry = rr[rr['id'] == resource_id]\n",
    "\n",
    "  assert len(resource_entry) == 1\n",
    "\n",
    "  resource_entry = resource_entry.squeeze()\n",
    "\n",
    "  current_entry = resource_entry\n",
    "  current_id = resource_entry['id']\n",
    "  found_stream = True\n",
    "  while (current_entry['resource_type'] != \"STREAM\"):\n",
    "    current_entry = rr[rr['parent_id'] == current_id]\n",
    "    current_id = current_entry['id']\n",
    "\n",
    "    if len(current_entry) == 0:\n",
    "      found_stream = False\n",
    "      break\n",
    "\n",
    "    if len(current_entry) != 1:\n",
    "      print(timestamp)\n",
    "      display(current_entry)\n",
    "      print([ts for ts in list(current_entry['timestamp']) if datetime.fromisoformat(ts).month == 9])\n",
    "      current_entry = current_entry[current_entry['last_discover'] == timestamp]\n",
    "      print(current_entry)\n",
    "      assert len(current_entry) == 1\n",
    "    current_entry = current_entry.squeeze()\n",
    "\n",
    "  if not found_stream:\n",
    "    print(f\"Didn't find stream for resource_id: {resource_id}\")\n",
    "    continue\n",
    "  \n",
    "  hash_id = current_entry['hash_id']\n",
    "  labels = rl[rl['hash_id'] == hash_id]\n",
    "  for label in labels.iterrows():\n",
    "    label = label[1]\n",
    "    label_name, label_value = label['name'], label['value']\n",
    "    label_arr = event['labels'].setdefault(label_name, [])\n",
    "    label_arr.append(label_value)\n",
    "  resource_history.append(event)\n",
    "\n",
    "  COUNT -= 1\n",
    "  if (COUNT <= 0):\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

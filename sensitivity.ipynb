{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8d196a1",
   "metadata": {},
   "source": [
    "### Sensitivity Experiments\n",
    "#### Methodologies for sensitvity assessment\n",
    "1) LLM based zero-shot/few-shot (\"Categorize sensitivity into these 'x' categories\")\n",
    "2) Transformer based PII extraction. Train sensitivity scorer on top of results (map score to category)\n",
    "3) TF-IDF for topics. Train sensitivity scorer on top of results (map score to category)\n",
    "#### Explainability\n",
    "1) LIME/SHAP to identify most influential features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into pandas DataFrames\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_dir = \"./csvs\"\n",
    "\n",
    "dfs = {}\n",
    "for filename in os.listdir(csv_dir):\n",
    "    path = csv_dir + \"/\" + filename\n",
    "    try:\n",
    "        name_no_ext = filename.split('.')[0]\n",
    "        dfs[name_no_ext] = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d393d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Print brief summary and show first few rows for each loaded dataframe\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24decebc",
   "metadata": {},
   "source": [
    "# Prepare Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe27590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LLM input\n",
    "\n",
    "llm_inputs = {}\n",
    "for _, label in dfs['resource_label'].iterrows():\n",
    "  label_type = label['name']\n",
    "  label_value = label['value']\n",
    "\n",
    "  input = llm_inputs.setdefault(label['hash_id'], {})\n",
    "  label_type_values = input.setdefault(label_type + \"s\", [])\n",
    "  label_type_values.append(label_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6508d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Remove inputs that don't have much information (likely indexing went wrong)\n",
    "valid_llm_inputs = {k: v for k, v in llm_inputs.items() if len(v.keys()) >= 2}\n",
    "\n",
    "NUM_INPUTS=30\n",
    "keys = list(valid_llm_inputs.keys())\n",
    "random.seed(42)\n",
    "random.shuffle(keys)\n",
    "keys = keys[:NUM_INPUTS]\n",
    "reduced_inputs = {k: v for k, v in llm_inputs.items() if k in keys}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89cf980",
   "metadata": {},
   "source": [
    "# LLM Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17cfe4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM query\n",
    "import json\n",
    "import openai\n",
    "\n",
    "# Load config\n",
    "openai_config_path = \"./configs/openai_standard.json\"\n",
    "with open(openai_config_path, \"r\") as f:\n",
    "  openai_config = json.loads(f.read())\n",
    "\n",
    "# LLM client\n",
    "client = openai.AzureOpenAI(\n",
    "  azure_endpoint=openai_config['endpoint'],\n",
    "  api_version=openai_config['api_version'],\n",
    "  api_key=openai_config['key']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert at assessing document sensitivity levels. You will receive document metadata in JSON format and must categorize each document's sensitivity.\n",
    "\n",
    "INPUT FORMAT:\n",
    "{\n",
    "  \"<document_id>\": {\n",
    "    \"document_types\": [\"<document_type_1>\", \"<document_type_n>\"],\n",
    "    \"primary_subjects\": [\"<primary_subject_1>\", \"<primary_subject_n>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "Note: Input may include additional fields such as \"summary\", \"names\", etc.\n",
    "\n",
    "SENSITIVITY CATEGORIES:\n",
    "Categorize each document into one of the following levels:\n",
    "- Public: Information suitable for public disclosure\n",
    "- Internal: Information for internal use only\n",
    "- Confidential: Sensitive business information with limited access\n",
    "- Restricted: Highly sensitive information (e.g., PII, financial data, trade secrets)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return valid JSON that can be parsed by Python's json.loads():\n",
    "{\n",
    "  \"document_id\": \"<document_id>\",\n",
    "  \"category\": \"<sensitivity_category>\",\n",
    "  \"confidence\": \"<confidence_percentage>\",\n",
    "  \"explanation\": \"<brief_justification>\"\n",
    "}\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{\n",
    "  \"document_id\": \"142\",\n",
    "  \"category\": \"Restricted\",\n",
    "  \"confidence\": \"80%\",\n",
    "  \"explanation\": \"Contains PII/PCI including SSNs and bank account information\"\n",
    "}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Output must be valid JSON parseable by json.loads()\n",
    "- Use consistent capitalization for category names (e.g., \"Restricted\" not \"RESTRICTED\")\n",
    "- Include all four fields: document_id, category, confidence, explanation\n",
    "- Provide clear, concise explanations for your categorization\"\"\"\n",
    "\n",
    "USER_PROMPT = f\"\"\"\n",
    "====START DOCUMENT INFORMATION====\n",
    "{reduced_inputs}\n",
    "====END DOCUMENT INFORMATION====\n",
    "\"\"\"\n",
    "\n",
    "print(USER_PROMPT)\n",
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc7b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt-4o-mini\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"developer\",\n",
    "      \"content\": SYSTEM_PROMPT\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": USER_PROMPT\n",
    "    }\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9753ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = completion.choices[0].message.content\n",
    "ans = ans[ans.find('['):ans.rfind(']')+1]\n",
    "json.loads(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LLM JSON output to CSV for Google Sheets\n",
    "# Assumes the LLM JSON contains the four fields: document_id, category, confidence, explanation\n",
    "# Expands the original input so each member of any array becomes its own column\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "\n",
    "# Ensure we have the LLM answer (ans) available\n",
    "if 'ans' not in globals():\n",
    "    if 'completion' in globals():\n",
    "        ans = completion.choices[0].message.content\n",
    "        if '[' in ans and ']' in ans:\n",
    "            ans = ans[ans.find('['):ans.rfind(']')+1]\n",
    "    else:\n",
    "        raise NameError(\"Neither 'ans' nor 'completion' found. Run the LLM completion cell first.\")\n",
    "\n",
    "# Parse JSON directly (we assume valid JSON with the four expected fields)\n",
    "results = json.loads(ans)\n",
    "\n",
    "# Normalize results into a list of records\n",
    "if isinstance(results, list):\n",
    "    records = results\n",
    "elif isinstance(results, dict):\n",
    "    # If dict maps document_id -> {fields}, convert to list\n",
    "    if all(isinstance(v, dict) for v in results.values()):\n",
    "        records = []\n",
    "        for k, v in results.items():\n",
    "            row = dict(v)\n",
    "            if 'document_id' not in row:\n",
    "                row['document_id'] = k\n",
    "            records.append(row)\n",
    "    else:\n",
    "        # Single-record dict with the four fields\n",
    "        records = [results]\n",
    "else:\n",
    "    raise ValueError(f\"Unexpected JSON structure: {type(results)}\")\n",
    "\n",
    "# Build DataFrame (expects columns: document_id, category, confidence, explanation)\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Attach and expand the original input for each document if available (prefer reduced_inputs)\n",
    "input_map = globals().get('reduced_inputs') or globals().get('llm_inputs') or {}\n",
    "\n",
    "# Build expanded input rows\n",
    "input_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    doc_id = row.get('document_id')\n",
    "    # try keys in multiple forms\n",
    "    found = None\n",
    "    for k in (doc_id, str(doc_id)):\n",
    "        if k in input_map:\n",
    "            found = input_map[k]\n",
    "            break\n",
    "    if found is None:\n",
    "        # try loose match by string equality\n",
    "        for k in input_map.keys():\n",
    "            if str(k) == str(doc_id):\n",
    "                found = input_map[k]\n",
    "                break\n",
    "    if found is None:\n",
    "        found = {}\n",
    "\n",
    "    expanded = {'document_id': doc_id}\n",
    "    # For each key in the input dict, expand lists into numbered columns,\n",
    "    # keep scalars as-is (strings/numbers)\n",
    "    for key, val in found.items():\n",
    "        if isinstance(val, (list, tuple)):\n",
    "            for i, v in enumerate(val):\n",
    "                expanded[f\"{key}_{i}\"] = v\n",
    "        else:\n",
    "            expanded[key] = val\n",
    "\n",
    "    input_rows.append(expanded)\n",
    "\n",
    "if input_rows:\n",
    "    inputs_df = pd.DataFrame(input_rows)\n",
    "    # Ensure both document_id are strings for a reliable merge\n",
    "    df['document_id'] = df['document_id'].astype(str)\n",
    "    inputs_df['document_id'] = inputs_df['document_id'].astype(str)\n",
    "    merged = df.merge(inputs_df, on='document_id', how='left')\n",
    "else:\n",
    "    # No inputs to expand; attach an empty placeholder\n",
    "    merged = df.copy()\n",
    "\n",
    "# Save CSV ready for Google Sheets upload\n",
    "out_path = Path(\"sensitivity_results_\" + str(datetime.now()) + \".csv\")\n",
    "merged.to_csv(out_path, index=False)\n",
    "print(f\"Saved CSV to: {out_path.resolve()} (shape: {merged.shape})\")\n",
    "\n",
    "# Display a preview\n",
    "display(merged.head())\n",
    "\n",
    "# Expose variables for interactive use\n",
    "sensitivity_results_df = merged\n",
    "sensitivity_results_csv = out_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598e0e5",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ed35ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install gliner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576808a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_to_string(inp):\n",
    "  str = \"| \"\n",
    "  for category, values in inp.items():\n",
    "    str += category + \": \"\n",
    "    for value in values[:-1]:\n",
    "      str += value + \", \"\n",
    "    str += values[-1]\n",
    "    str += \" | \"\n",
    "  return str\n",
    "\n",
    "INDEX = 29\n",
    "text = f\"\"\"\n",
    "{json_to_string(list(reduced_inputs.values())[INDEX])}\n",
    "\"\"\"\n",
    "\n",
    "print(list(reduced_inputs.keys())[INDEX])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5782bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gliner import GLiNER\n",
    "\n",
    "model = GLiNER.from_pretrained(\"urchade/gliner_multi_pii-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2541a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"booking number\", \"personally identifiable information\", \"driver licence\", \"person\", \"book\", \"full address\", \"company\", \"actor\", \"character\", \"email\", \"passport number\", \"Social Security Number\", \"phone number\", \"financial data\"]\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity)\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2b48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers==4.50.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e17b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"iiiorg/piiranha-v1-detect-personal-information\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105389e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "# Get the model predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9039b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted labels\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "# Convert token predictions to word predictions\n",
    "encoded_inputs = tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "offset_mapping = encoded_inputs['offset_mapping']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a19ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_pii(text, aggregate_redaction=True):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Get the model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted labels\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    # Convert token predictions to word predictions\n",
    "    encoded_inputs = tokenizer.encode_plus(text, return_offsets_mapping=True, add_special_tokens=True)\n",
    "    offset_mapping = encoded_inputs['offset_mapping']\n",
    "\n",
    "    masked_text = list(text)\n",
    "    is_redacting = False\n",
    "    redaction_start = 0\n",
    "    current_pii_type = ''\n",
    "\n",
    "    for i, (start, end) in enumerate(offset_mapping):\n",
    "        if start == end:  # Special token\n",
    "            continue\n",
    "\n",
    "        label = predictions[0][i].item()\n",
    "        if label != model.config.label2id['O']:  # Non-O label\n",
    "            pii_type = model.config.id2label[label]\n",
    "            if not is_redacting:\n",
    "                is_redacting = True\n",
    "                redaction_start = start\n",
    "                current_pii_type = pii_type\n",
    "            elif not aggregate_redaction and pii_type != current_pii_type:\n",
    "                # End current redaction and start a new one\n",
    "                apply_redaction(masked_text, redaction_start, start, current_pii_type, aggregate_redaction)\n",
    "                redaction_start = start\n",
    "                current_pii_type = pii_type\n",
    "        else:\n",
    "            if is_redacting:\n",
    "                apply_redaction(masked_text, redaction_start, end, current_pii_type, aggregate_redaction)\n",
    "                is_redacting = False\n",
    "\n",
    "    # Handle case where PII is at the end of the text\n",
    "    if is_redacting:\n",
    "        apply_redaction(masked_text, redaction_start, len(masked_text), current_pii_type, aggregate_redaction)\n",
    "\n",
    "    return ''.join(masked_text)\n",
    "\n",
    "def apply_redaction(masked_text, start, end, pii_type, aggregate_redaction):\n",
    "    for j in range(start, end):\n",
    "        masked_text[j] = ''\n",
    "    if aggregate_redaction:\n",
    "        masked_text[start] = '[redacted]'\n",
    "    else:\n",
    "        masked_text[start] = f'[{pii_type}]'\n",
    "\n",
    "print(\"Aggregated redaction:\")\n",
    "masked_example_aggregated = mask_pii(text, aggregate_redaction=True)\n",
    "print(masked_example_aggregated)\n",
    "\n",
    "print(\"\\nDetailed redaction:\")\n",
    "masked_example_detailed = mask_pii(text, aggregate_redaction=False)\n",
    "print(masked_example_detailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4061564",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"names\", \"personally identifiable information\", \"financial information\", \"financial figures\", \"revenue\"]\n",
    "entities = model.predict_entities(text, labels)\n",
    "\n",
    "for entity in entities:\n",
    "    print(entity[\"text\"], \"=>\", entity[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d393d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "# Print brief summary and show first few rows for each loaded dataframe\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert at assessing document sensitivity levels. You will receive document metadata in JSON format and must categorize each document's sensitivity.\n",
    "\n",
    "INPUT FORMAT:\n",
    "{\n",
    "  \"<document_id>\": {\n",
    "    \"document_types\": [\"<document_type_1>\", \"<document_type_n>\"],\n",
    "    \"primary_subjects\": [\"<primary_subject_1>\", \"<primary_subject_n>\"]\n",
    "  }\n",
    "}\n",
    "\n",
    "Note: Input may include additional fields such as \"summary\", \"names\", etc.\n",
    "\n",
    "SENSITIVITY CATEGORIES:\n",
    "Categorize each document into one of the following levels:\n",
    "- Public: Information suitable for public disclosure\n",
    "- Internal: Information for internal use only\n",
    "- Confidential: Sensitive business information with limited access\n",
    "- Restricted: Highly sensitive information (e.g., PII, financial data, trade secrets)\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return valid JSON that can be parsed by Python's json.loads():\n",
    "{\n",
    "  \"document_id\": \"<document_id>\",\n",
    "  \"category\": \"<sensitivity_category>\",\n",
    "  \"confidence\": \"<confidence_percentage>\",\n",
    "  \"explanation\": \"<brief_justification>\"\n",
    "}\n",
    "\n",
    "EXAMPLE OUTPUT:\n",
    "{\n",
    "  \"document_id\": \"142\",\n",
    "  \"category\": \"Restricted\",\n",
    "  \"confidence\": \"80%\",\n",
    "  \"explanation\": \"Contains PII/PCI including SSNs and bank account information\"\n",
    "}\n",
    "\n",
    "CRITICAL REQUIREMENTS:\n",
    "- Output must be valid JSON parseable by json.loads()\n",
    "- Use consistent capitalization for category names (e.g., \"Restricted\" not \"RESTRICTED\")\n",
    "- Include all four fields: document_id, category, confidence, explanation\n",
    "- Provide clear, concise explanations for your categorization\"\"\"\n",
    "\n",
    "USER_PROMPT = f\"\"\"\n",
    "====START DOCUMENT INFORMATION====\n",
    "{reduced_inputs}\n",
    "====END DOCUMENT INFORMATION====\n",
    "\"\"\"\n",
    "\n",
    "print(USER_PROMPT)\n",
    "print(SYSTEM_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9753ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = completion.choices[0].message.content\n",
    "ans = ans[ans.find('['):ans.rfind(']')+1]\n",
    "json.loads(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf42e87",
   "metadata": {},
   "source": [
    "# Contextualizing the data- connecting to audit log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8faeb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "rar = dfs['resource_auditrecord']\n",
    "rar[rar['audited_id'] == 2443586]['operation'].unique()\n",
    "rar[rar['audited_id'] == 2443586][rar['operation'] == \"MODIFIED\"].sort_values(by=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "82ae3599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>params</th>\n",
       "      <th>name</th>\n",
       "      <th>integration</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>resource_id</th>\n",
       "      <th>uniqueness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>2444296</td>\n",
       "      <td>2</td>\n",
       "      <td>2025-09-27 21:01:33.803+00</td>\n",
       "      <td>\\x800495b0000000000000008c12696e74656772617469...</td>\n",
       "      <td>1Q2yTl0eXejDDG0RJEgQv8TL8u7JA7nj3</td>\n",
       "      <td>GOOGLE:terasky-production</td>\n",
       "      <td>NaN</td>\n",
       "      <td>814273.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  seqnum                   timestamp  \\\n",
       "800  2444296       2  2025-09-27 21:01:33.803+00   \n",
       "\n",
       "                                                params  \\\n",
       "800  \\x800495b0000000000000008c12696e74656772617469...   \n",
       "\n",
       "                                  name                integration  parent_id  \\\n",
       "800  1Q2yTl0eXejDDG0RJEgQv8TL8u7JA7nj3  GOOGLE:terasky-production        NaN   \n",
       "\n",
       "     resource_id  uniqueness  \n",
       "800     814273.0           3  "
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rrn = dfs['resource_resourcenode']\n",
    "rrn[rrn['id'] == 2444296]\n",
    "# rrn[rrn['resource_id'] == 813557]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0c247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through resource_label_assignees\n",
    "\n",
    "rla = dfs[\"resource_label_assignees\"]\n",
    "rla[rla['hash_id'] == 81617].head()\n",
    "# rla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "e7637189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>params</th>\n",
       "      <th>resource_type</th>\n",
       "      <th>priority</th>\n",
       "      <th>hash_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>last_discover</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>297032</th>\n",
       "      <td>814273</td>\n",
       "      <td>6</td>\n",
       "      <td>2025-09-27 21:01:33.803+00</td>\n",
       "      <td>\\x800495b0000000000000008c12696e74656772617469...</td>\n",
       "      <td>FILE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-09-28 18:23:08.010107+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  seqnum                   timestamp  \\\n",
       "297032  814273       6  2025-09-27 21:01:33.803+00   \n",
       "\n",
       "                                                   params resource_type  \\\n",
       "297032  \\x800495b0000000000000008c12696e74656772617469...          FILE   \n",
       "\n",
       "        priority  hash_id  parent_id                  last_discover  \n",
       "297032       NaN      NaN        NaN  2025-09-28 18:23:08.010107+00  "
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr = dfs['resource_resource']\n",
    "rr[rr['id'] == 814273.0].sort_values(by=\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rh = dfs['resource_hash']\n",
    "rh[rh['id'] == 81531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56c5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_times = rar[rar['audited_id'] == 2443586]['timestamp']\n",
    "\n",
    "rr = dfs['resource_resource']\n",
    "rr = rr[rr['id'] == 813557]\n",
    "\n",
    "# display(rr)\n",
    "# target_time = \"2025-10-08 10:21:52.542+00\"\n",
    "# target_time = datetime.fromisoformat(target_time)\n",
    "timestamps = list(rr['timestamp'])\n",
    "\n",
    "for audit_time in audit_times:\n",
    "  if (audit_time in timestamps):\n",
    "    display(rar[rar['audited_id'] == 2443586][rar['timestamp'] == audit_time])\n",
    "    print(\"FOUND\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['resource_label'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d4c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rr[rr['id'] == 821690]\n",
    "# len(df[df['resource_type'] == \"STREAM\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "104d78c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n",
      "Found more than one resource with ID:  Series([], Name: resource_id, dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# Collecting all of the streams topics/access metadata under one parent resource\n",
    "# 1) Iterate through audit log\n",
    "\n",
    "'''\n",
    "\"resource_id\": {\n",
    "  metadata...\n",
    "  history: [\n",
    "    operation\n",
    "    time_of_access\n",
    "    ip\n",
    "    resource_information\n",
    "  ]\n",
    "}\n",
    "'''\n",
    "resources = {}\n",
    "\n",
    "for ar in dfs['resource_auditrecord'].iterrows():\n",
    "  ar = ar[1]\n",
    "  timestamp, operation, location, ip, user_id, rn_id = ar['timestamp'], ar['operation'], ar['geolocation'], ar['client_ip'], ar['user_id'], ar['audited_id']\n",
    "  resource_info = resources.setdefault(rn_id, {})\n",
    "  resource_history = resource_info.setdefault(\"history\", [])\n",
    "\n",
    "  # Info from audit record\n",
    "  event = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"operation\": operation,\n",
    "    \"location\": location,\n",
    "    \"ip\": ip,\n",
    "    \"user_id\": user_id,\n",
    "    \"labels\": {} # To be filled out next\n",
    "  }\n",
    "\n",
    "  # If the event isn't a modification, no need to fetch any labels. Just add event and move on\n",
    "  if operation != \"MODIFIED\" and operation != \"FILE_UPLOADED\":\n",
    "    resource_history.append(event)\n",
    "    continue\n",
    "\n",
    "  # This is a MODIFIED event, lets fetch the labels from DB\n",
    "  # Get the resource ID from resource node\n",
    "  rrn = dfs['resource_resourcenode']\n",
    "  resource_node = rrn[rrn['id'] == rn_id]\n",
    "  resource_id = resource_node['resource_id']\n",
    "\n",
    "  # Should only be one...\n",
    "  if len(resource_id) != 1:\n",
    "    print(\"Found more than one resource with ID: \", resource_id)\n",
    "    continue\n",
    "  resource_id = resource_id.item()\n",
    "\n",
    "  # Get the dataframes we need\n",
    "  rr, rl, rla = dfs['resource_resource'], dfs['resource_label'], dfs['resource_label_assignees']\n",
    "\n",
    "  # Grab the resource reference by the audit log event\n",
    "  resource = rr[rr['id'] == resource_id]\n",
    "  if len(resource) != 1:\n",
    "    print(\"Resource length greater than 1: \", resource)\n",
    "    continue\n",
    "\n",
    "  # Work our way down to the relevant STREAM, or in other words actual data, relevant for this audit log event\n",
    "  file_or_stream = resource.squeeze()\n",
    "\n",
    "  if file_or_stream['resource_type'] != \"STREAM\":\n",
    "    # Must be a file\n",
    "    assert file_or_stream['resource_type'] == \"FILE\"\n",
    "    file = file_or_stream\n",
    "    streams = rr[rr['parent_id'] == file['id']]\n",
    "    # Get all the streams before event timestamp\n",
    "    # streams = streams[streams['timestamp'] <= timestamp]\n",
    "    \n",
    "    if len(streams) == 0:\n",
    "      # print(\"No streams for id: \", file['id'])\n",
    "      continue\n",
    "    # Pick the last one!\n",
    "    if len(streams) == 1:\n",
    "      stream = streams.iloc[0]\n",
    "    else:\n",
    "      filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "      stream = filtered_streams.iloc[0] if len(filtered_streams) > 0 else streams.iloc[0]\n",
    "  else:\n",
    "    stream = file_or_stream\n",
    "  \n",
    "  hash_id = stream.squeeze()['hash_id']\n",
    "  labels = rl[rl['hash_id'] == hash_id]\n",
    "  for label in labels.iterrows():\n",
    "    label = label[1]\n",
    "    label_name, label_value = label['name'], label['value']\n",
    "    label_arr = event['labels'].setdefault(label_name, [])\n",
    "    label_arr.append(label_value)\n",
    "  resource_history.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "e2c375c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"audit_events_for_model.pkl\", \"wb\") as f:\n",
    "  pickle.dump(resource, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc284710",
   "metadata": {},
   "source": [
    "# Ask LLM to understand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "ac7e7c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_event = sorted(list(resources.values())[0]['history'], key=lambda x: x['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404c45d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

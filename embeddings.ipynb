{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb5b98b",
   "metadata": {},
   "source": [
    "# Representing Users and Files\n",
    "#### Design Document: https://docs.google.com/document/d/1F84Nj3IQ-f_36bmmsOTuOo9u65gfD1WU5LKdnw8ShcY/edit?tab=t.x97j5jy1kop1#heading=h.1vu1g9fe3ujo\n",
    "#### Optimizations:\n",
    "- Re-ranking topics\n",
    "- More information in file embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988742d",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af224c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install openai\n",
    "! pip install sentence-transformers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9430db1",
   "metadata": {},
   "source": [
    "# Utilities/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files representing database into pandas DataFrames\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "CSV_DIR = \"./csvs\"\n",
    "\n",
    "DFS = {}\n",
    "for filename in os.listdir(CSV_DIR):\n",
    "    path = CSV_DIR + \"/\" + filename\n",
    "    try:\n",
    "        name_no_ext = filename.split('.')[0]\n",
    "        DFS[name_no_ext] = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {path}: {e}\")\n",
    "\n",
    "# Print brief summary and show first few rows for each loaded dataframe\n",
    "from IPython.display import display\n",
    "\n",
    "for name, df in DFS.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    display(df.head())\n",
    "\n",
    "UNIQUE_OPS = DFS['resource_auditrecord']['operation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89901229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Now for this session\n",
    "NOW = datetime.now(timezone.utc)\n",
    "\n",
    "# Cached embeddings directory\n",
    "CACHE_DIR = \"./pickle/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache any heavy embeddings or mappings we compute.\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def _most_recent_file(pattern):\n",
    "  # Return the most recently modified file in cache_dir matching the glob pattern, or None if none found\n",
    "  matches = glob.glob(os.path.join(CACHE_DIR, pattern))\n",
    "  if not matches:\n",
    "    return None\n",
    "  return max(matches, key=os.path.getmtime)\n",
    "\n",
    "def get_cache():\n",
    "  audit_to_file_mapping_file_path = _most_recent_file(\"audit_to_file_mapping_*.pkl\")\n",
    "  topic_embeddings_file_path = _most_recent_file(\"topic_embeddings_bge-large-zh-v1.5_*.pkl\")\n",
    "  file_embeddings_file_path = _most_recent_file(\"file_embeddings_*.pkl\")\n",
    "  user_embeddings_file_path = _most_recent_file(\"user_embeddings_*.pkl\")\n",
    "\n",
    "  audit_to_file_mapping, embeddings_cache, files, users = {}, {}, {}, {}\n",
    "\n",
    "  if audit_to_file_mapping_file_path and os.path.exists(audit_to_file_mapping_file_path):\n",
    "    with open(audit_to_file_mapping_file_path, \"rb\") as p:\n",
    "      audit_to_file_mapping = pickle.load(p)\n",
    "\n",
    "  if topic_embeddings_file_path and os.path.exists(topic_embeddings_file_path):\n",
    "    with open(topic_embeddings_file_path, \"rb\") as p:\n",
    "      embeddings_cache = pickle.load(p)\n",
    "\n",
    "  if file_embeddings_file_path and os.path.exists(file_embeddings_file_path):\n",
    "    with open(file_embeddings_file_path, \"rb\") as p:\n",
    "      files = pickle.load(p)\n",
    "\n",
    "  if user_embeddings_file_path and os.path.exists(user_embeddings_file_path):\n",
    "    with open(user_embeddings_file_path, \"rb\") as p:\n",
    "      users = pickle.load(p)\n",
    "\n",
    "  print(f\"Audit to file mapping size: {len(audit_to_file_mapping)}\")\n",
    "  print(f\"Topic embeddings cache size: {len(embeddings_cache) if embeddings_cache is not None else 0}\")\n",
    "  print(f\"Files size: {len(files) if files is not None else 0}\")\n",
    "  print(f\"Users size: {len(users) if users is not None else 0}\")\n",
    "\n",
    "  return audit_to_file_mapping, embeddings_cache, files, users\n",
    "\n",
    "\n",
    "def save_to_cache(audit_to_file_mapping, embeddings_cache, files, users):\n",
    "  # Ensure cache dir exists\n",
    "  os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "  save_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "  with open(os.path.join(CACHE_DIR, f\"topic_embeddings_bge-large-zh-v1.5_{save_time}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(embeddings_cache, f)\n",
    "  with open(os.path.join(CACHE_DIR, f\"file_embeddings_{save_time}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(files, f)\n",
    "  with open(os.path.join(CACHE_DIR, f\"user_embeddings_{save_time}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(users, f)\n",
    "  with open(os.path.join(CACHE_DIR, f\"audit_to_file_mapping_{save_time}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(audit_to_file_mapping, f)\n",
    "\n",
    "AUDIT_TO_FILE_MAPPING, EMBEDDINGS_CACHE, FILES, USERS = get_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c842ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database searching utilities\n",
    "from functools import cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Getting audit ids which are associated with a file we have information for\n",
    "# This way we have a much smaller list of audit logs to parse when we are\n",
    "# calculating user embeddings\n",
    "@cache\n",
    "def get_valid_audit_ids():\n",
    "  rr, rrn, ra = DFS[\"resource_resource\"], DFS[\"resource_resourcenode\"], DFS[\"resource_auditrecord\"]\n",
    "  valid_resource_ids = set()\n",
    "  print(\"Get valid resources\")\n",
    "  for row in tqdm(rr.iterrows(), total=len(rr)):\n",
    "    row_info = row[1]\n",
    "    id = row_info['id']\n",
    "\n",
    "    if len(rr[rr['parent_id'] == id]) != 0:\n",
    "      valid_resource_ids.add(id)\n",
    "\n",
    "  valid_resource_node_ids = set()\n",
    "  print(\"Get valid resource nodes\")\n",
    "  for row in tqdm(rrn.iterrows(), total=len(rrn)):\n",
    "    row_info = row[1]\n",
    "    if row_info['resource_id'] in valid_resource_ids:\n",
    "      valid_resource_node_ids.add(row_info['id'])\n",
    "\n",
    "  valid_audit_ids = set()\n",
    "  ra.head()\n",
    "  print(\"Get valid audit ids\")\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "    if row_info['audited_id'] in valid_resource_node_ids:\n",
    "      valid_audit_ids.add(row_info['id'])\n",
    "  return valid_audit_ids, valid_resource_node_ids, valid_resource_ids\n",
    "\n",
    "def get_stream(audit_id, file_id, streams, timestamp, operation):\n",
    "  # File has been changed in some way, we want to find the first STREAM that is after the audit log timestamp\n",
    "  should_check_after = operation in ['MODIFIED', 'FILE_UPLOADED', 'RENAMED']\n",
    "  \n",
    "  # 1 stream, always pick it\n",
    "  if len(streams) == 1:\n",
    "    stream = streams.iloc[0]\n",
    "  else:\n",
    "    streams = streams.sort_values(by=\"timestamp\")\n",
    "    if should_check_after:\n",
    "      filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "      if len(filtered_streams) == 0:\n",
    "        # Fallback to filtering opposite way\n",
    "        filtered_streams = streams[streams['timestamp'] <= timestamp]\n",
    "        stream = filtered_streams.iloc[-1]\n",
    "      else:\n",
    "        stream = filtered_streams.iloc[0]\n",
    "    else:\n",
    "      filtered_streams = streams[streams['timestamp'] <= timestamp]\n",
    "      if len(filtered_streams) == 0:\n",
    "        # Fallback to filtering opposite way\n",
    "        filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "        stream = filtered_streams.iloc[0]\n",
    "      else:\n",
    "        stream = filtered_streams.iloc[-1]\n",
    "  return stream\n",
    "\n",
    "def get_file_hash_as_of_audit(audit_id, operation, timestamp):\n",
    "  rrn, rr = DFS['resource_resourcenode'], DFS['resource_resource']\n",
    "  resource_node = rrn[rrn['id'] == audit_id]\n",
    "  resource_id = resource_node['resource_id']\n",
    "\n",
    "  # Assume only one resource id associated with audit id?\n",
    "  if len(resource_id) == 0:\n",
    "    raise ValueError(f\"No resource id found. Audit id: {audit_id}\")\n",
    "  resource_id = resource_id.iloc[0]\n",
    "\n",
    "  # Grab the resource reference by the audit log event\n",
    "  resource = rr[rr['id'] == resource_id]\n",
    "  if len(resource) == 0:\n",
    "    raise ValueError(f\"No resource found: Audit id: {audit_id}\")\n",
    "\n",
    "  # Work our way down to the relevant STREAM, or in other words actual data, relevant for this audit log event\n",
    "  file_or_stream = resource.iloc[0]\n",
    "\n",
    "  if file_or_stream['resource_type'] != \"STREAM\":\n",
    "      # Must be a file\n",
    "      assert file_or_stream['resource_type'] == \"FILE\"\n",
    "      file = file_or_stream\n",
    "      streams = rr[rr['parent_id'] == file['id']]\n",
    "      if len(streams) is None:\n",
    "        # If no streams, at least return file id\n",
    "        return file['id'], False\n",
    "      stream = get_stream(audit_id, file['id'], streams, timestamp, operation)\n",
    "      hash_id = stream['hash_id']\n",
    "  else:\n",
    "      # If stream, directly get hash id\n",
    "      hash_id = file_or_stream['hash_id']\n",
    "\n",
    "  return hash_id, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb592e66",
   "metadata": {},
   "source": [
    "# User/File Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'BAAI/bge-large-zh-v1.5'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE/TOPIC embeddings (get topic embeddings as we calculate file embeddings)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def average_embeddings(embeddings):\n",
    "  return np.mean(embeddings, axis=0)\n",
    "\n",
    "def get_embedding_for_text(text: str):\n",
    "  text = text.lower()\n",
    "\n",
    "  if text in EMBEDDINGS_CACHE:\n",
    "    return EMBEDDINGS_CACHE[text]\n",
    "  \n",
    "  EMBEDDINGS_CACHE[text] = model.encode(text, normalize_embeddings=True) # normalize for cosine similarity\n",
    "  return EMBEDDINGS_CACHE[text]\n",
    "\n",
    "def get_file_embeddings():\n",
    "  files = {}\n",
    "  rl = DFS['resource_label']\n",
    "  rl = rl[rl['name'] == \"topic\"]\n",
    "\n",
    "  print(\"Working through resource labels\")\n",
    "  for row in tqdm(rl.iterrows(), total=len(rl)):\n",
    "\n",
    "    resource_info = row[1]\n",
    "    file_info = files.setdefault(resource_info['hash_id'], {\n",
    "      \"labels\": [],\n",
    "      \"embedding\": []\n",
    "    })\n",
    "\n",
    "    if (resource_info[\"name\"] != \"topic\"):\n",
    "      continue\n",
    "\n",
    "    file_info[\"labels\"].append({\n",
    "      \"id\": resource_info[\"id\"],\n",
    "      \"name\": resource_info[\"value\"]\n",
    "    })\n",
    "\n",
    "  print(\"Average topic embeddings for files\")\n",
    "  for _, info in tqdm(files.items(), total=len(files)):\n",
    "    info['embedding'] = average_embeddings([get_embedding_for_text(label['name']) for label in info['labels']])\n",
    "  return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER embeddings, for each time-weight the relevant file-embeddings. Create dict of {user_id: embedding}\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def aggregate_file_embeddings_per_user(valid_audit_ids):\n",
    "  users = {}\n",
    "  ar = DFS['resource_auditrecord']\n",
    "\n",
    "  ar = ar[ar['id'].isin(valid_audit_ids)]\n",
    "  \n",
    "  bad_resource_ids = set()\n",
    "  good_resource_ids = set()\n",
    "\n",
    "  print(\"Processing audit log records:\")\n",
    "  for row in tqdm(ar.iterrows(), total=len(ar)):\n",
    "    \n",
    "    row_info = row[1]\n",
    "    if row_info['audited_id'] in bad_resource_ids:\n",
    "      continue\n",
    "    \n",
    "    user_info = users.setdefault(row_info['user_id'], {\n",
    "      \"file_infos\": []\n",
    "    })\n",
    "    time_of_operation = row_info[\"timestamp\"]\n",
    "    try:\n",
    "      # Get most relevant file version as of audit time\n",
    "      hash_id, found_streams = get_file_hash_as_of_audit(row_info['audited_id'], row_info['operation'], time_of_operation)\n",
    "    except Exception as e:\n",
    "      # print(f\"Exception caught: {e}\")\n",
    "      bad_resource_ids.add(row_info['audited_id'])\n",
    "      continue\n",
    "\n",
    "    # Cache audit id to file mapping for later metadata processing, even when no streams found\n",
    "    AUDIT_TO_FILE_MAPPING[row_info['id']] = hash_id\n",
    "\n",
    "    if hash_id not in FILES:\n",
    "      # print(f\"Didn't find {hash_id} in file, audit id: {row_info['audited_id']}, must have failed topic extraction\")\n",
    "      continue\n",
    "\n",
    "    good_resource_ids.add(row_info['audited_id'])\n",
    "\n",
    "    # If successfully got hash, lookup in files table and add to user info\n",
    "    file_info = FILES[hash_id]\n",
    "    file_embedding = file_info[\"embedding\"]\n",
    "    user_info['file_infos'].append({\n",
    "      \"timestamp\": datetime.fromisoformat(time_of_operation),\n",
    "      \"embedding\": file_embedding\n",
    "    })\n",
    "\n",
    "  # print(f\"Success: {len(good_resource_ids)}, Error: {len(bad_resource_ids)}\")\n",
    "  return users\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_user_embedding(user_info, tau=60*60*24*30): \n",
    "  '''\n",
    "  1) Default tau to 1 month, in other words, we lose 37% of old information if \n",
    "     a month has passed since last indexing. Exponential decay.\n",
    "  2) Unit of time is seconds\n",
    "  '''\n",
    "\n",
    "  user_embedding = np.array(user_info['file_infos'][0]['embedding'])\n",
    "  embedding_time = user_info['file_infos'][0]['timestamp']\n",
    "\n",
    "  # Build up the index recursively\n",
    "  for file_info in user_info['file_infos'][1:]:\n",
    "    # Get difference from last time and update time variable\n",
    "    d_t = file_info['timestamp'] - embedding_time\n",
    "    d_t = d_t.total_seconds()\n",
    "    embedding_time = file_info['timestamp']\n",
    "\n",
    "    assert d_t >= 0, f\"User file access times must be ordered: from - {embedding_time}, to - {file_info['timestamp']}\"\n",
    "\n",
    "    # Apply the weighting\n",
    "    # d_t and tau should both be in seconds\n",
    "    alpha = 1 - math.exp(-d_t / tau)\n",
    "    user_embedding = alpha * np.array(file_info['embedding']) + (1 - alpha) * user_embedding\n",
    "\n",
    "  return user_embedding\n",
    "\n",
    "def get_user_embeddings(users):\n",
    "  # 2.1) Time weight the file embeddings\n",
    "  print(\"Calculating user embeddings: \")\n",
    "  for user, user_info in tqdm(users.items()):\n",
    "    user_info['file_infos'] = sorted(user_info['file_infos'], key=lambda event: event['timestamp'])\n",
    "    if len(user_info['file_infos']) == 0: \n",
    "      print(f\"User has no files {user}\")\n",
    "      continue\n",
    "    user_info[\"embedding\"] = get_user_embedding(user_info)\n",
    "  return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(AUDIT_TO_FILE_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS[list(USERS.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500456c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES, USERS, AUDIT_TO_FILE_MAPPING, EMBEDDINGS_CACHE = None, None, dict(), EMBEDDINGS_CACHE\n",
    "FILES = get_file_embeddings()\n",
    "\n",
    "# This step is an optimization, don't bother processing audit ids associated with files that haven't been indexed\n",
    "valid_audit_ids, _, _ = get_valid_audit_ids()\n",
    "USERS_AGG = aggregate_file_embeddings_per_user(valid_audit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1003acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS = get_user_embeddings(USERS_AGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_cache(AUDIT_TO_FILE_MAPPING, EMBEDDINGS_CACHE, FILES, USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fc145",
   "metadata": {},
   "source": [
    "# Meta Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baacc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta helpers\n",
    "import math\n",
    "from collections import Counter\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def get_interarrivals(timestamps):\n",
    "  interarrivals = []\n",
    "  if len(timestamps) <= 1:\n",
    "    return []\n",
    "  prev_timestamp = timestamps[0]\n",
    "  for next_timestamp in timestamps[1:]:\n",
    "    diff = (next_timestamp - prev_timestamp).total_seconds()\n",
    "    assert diff >= 0, f\"not ordered: start_time: {prev_timestamp}, next_time: {next_timestamp}\"\n",
    "    interarrivals.append(diff)\n",
    "    prev_timestamp = next_timestamp\n",
    "  return interarrivals\n",
    "\n",
    "def get_mit(interarrivals):\n",
    "  return sum(interarrivals) / len(interarrivals) if len(interarrivals) > 0 else -1\n",
    "\n",
    "def get_sit(interarrivals, mean):\n",
    "  deviations = [math.pow(i - mean, 2) for i in interarrivals]\n",
    "  if len(deviations) == 0:\n",
    "    return -1\n",
    "  return math.sqrt(sum(deviations) / len(deviations))\n",
    "\n",
    "def get_fft_info(buckets):\n",
    "  # Calculate top 3 frequencies/amplitudes/phases of time series using FFT\n",
    "  bucket_arr = np.array(buckets)\n",
    "  fft_result = np.fft.fft(bucket_arr)\n",
    "  fft_freqs = np.fft.fftfreq(len(bucket_arr), d=1)\n",
    "  amplitudes = np.abs(fft_result)\n",
    "  phases = np.angle(fft_result)\n",
    "  # Ignore the zero frequency (DC component)\n",
    "  indices = np.argsort(amplitudes[1:])[::-1][:3] + 1 if len(amplitudes) > 1 else []\n",
    "  return [(fft_freqs[i], amplitudes[i], phases[i]) for rank, i in enumerate(indices)]\n",
    "\n",
    "def get_burstiness(interarrivals, meta):\n",
    "  # TODO\n",
    "  pass\n",
    "\n",
    "def get_time_based_meta(start, end, interval, meta, operation, time_str, times):\n",
    "  # Sort datetimes so it's O(n) operation to create buckets\n",
    "  buckets = []\n",
    "  current = start\n",
    "  idx = 0\n",
    "  n = len(times)\n",
    "  while current < end:\n",
    "      next_bucket = current + interval\n",
    "      count = 0\n",
    "      # Count how many times fall into [current, next_bucket)\n",
    "      while idx < n and times[idx] < next_bucket:\n",
    "          if times[idx] >= current:\n",
    "              count += 1\n",
    "          idx += 1\n",
    "      buckets.append(count)\n",
    "      current = next_bucket\n",
    "  \n",
    "  for i, bucket in enumerate(buckets):\n",
    "    meta['times'][f'{operation}_{time_str}_{i}'] = bucket\n",
    "  fft_infos = get_fft_info(buckets)\n",
    "  for i, info in enumerate(fft_infos):\n",
    "    meta['times'][f'{operation}_{time_str}_freq_{i}'] = info[0]\n",
    "    meta['times'][f'{operation}_{time_str}_amp_{i}'] = info[1]\n",
    "    meta['times'][f'{operation}_{time_str}_phase_{i}'] = info[2]\n",
    "\n",
    "def get_entropy(values, normalized=True):\n",
    "  if not values:\n",
    "    return 0.0\n",
    "  counts = Counter(values)\n",
    "  total = sum(counts.values())\n",
    "  probs = [cnt / total for cnt in counts.values()]\n",
    "  H = -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "  if not normalized:\n",
    "      return H\n",
    "\n",
    "  k = len(counts)\n",
    "  if k <= 1:\n",
    "      return 0.0\n",
    "  return H / math.log2(k)\n",
    "\n",
    "def setup_metadata_from_audit_log(metadata, row_info):\n",
    "  # Add location information\n",
    "  locs = metadata.setdefault('locs', list())\n",
    "  locs.append(f'{row_info['geolocation']}-{row_info['client_ip']}')\n",
    "\n",
    "  # Add timestamps for operations\n",
    "  operation = row_info['operation']\n",
    "  times = metadata.setdefault('times', {})\n",
    "\n",
    "  all_operations = times.setdefault('all_ops', [])\n",
    "  operation_times = times.setdefault(operation, [])\n",
    "  event = {\n",
    "     \"timestamp\": datetime.fromisoformat(row_info['timestamp']),\n",
    "     \"user\": row_info[\"user_id\"]\n",
    "  }\n",
    "  # Per operation\n",
    "  operation_times.append(event)\n",
    "  # All operations\n",
    "  all_operations.append(event)\n",
    "\n",
    "def fill_metadata_stats_for_file_or_users(metadata, users, num_top=3):\n",
    "  sorted_ops = [o['timestamp'] for o in sorted(metadata['times']['all_ops'], key=lambda op: op['timestamp'])]\n",
    "  interarrivals = get_interarrivals(sorted_ops)\n",
    "  # mean interarrival time\n",
    "  metadata['mit'] = sum(interarrivals) / len(interarrivals) if len(interarrivals) > 0 else -1\n",
    "  # std interarrival time\n",
    "  metadata['sit'] = get_sit(interarrivals, metadata['mit'])\n",
    "  # recency\n",
    "  metadata['recency'] = (NOW - sorted_ops[-1]).total_seconds()\n",
    "\n",
    "  # user distribution entropy\n",
    "  metadata['ud_entropy'] = get_entropy(users)\n",
    "  # unique users\n",
    "  metadata['unique_users'] = len(set(users))\n",
    "  # top users\n",
    "  counter = Counter(users)\n",
    "  top_users = counter.most_common(num_top)\n",
    "  for i, user in enumerate(top_users):\n",
    "    metadata[f'top_user_{i}'] = user[0]\n",
    "    metadata[f'top_user_{i}_per'] = user[1] / len(users)\n",
    "\n",
    "  # unique locations\n",
    "  metadata['unique_locs'] = len(set(metadata['locs']))\n",
    "\n",
    "  # top locations\n",
    "  counter = Counter(metadata['locs'])\n",
    "  top_locs = counter.most_common(num_top)\n",
    "  for i, loc in enumerate(top_locs):\n",
    "    metadata[f'top_loc_{i}'] = loc[0]\n",
    "    metadata[f'top_loc_{i}_per'] = loc[1] / len(metadata['locs'])\n",
    "\n",
    "  # time meta\n",
    "  for OP in UNIQUE_OPS:\n",
    "    if not metadata.get(\"times\"):\n",
    "      break\n",
    "    op_times = []\n",
    "    if metadata[\"times\"].get(OP):\n",
    "      op_times = [t['timestamp'] for t in sorted([op for op in metadata[\"times\"][OP]], key=lambda operation: operation[\"timestamp\"])]\n",
    "\n",
    "    \n",
    "    # time buckets and FFT for specific operations\n",
    "    get_time_based_meta(NOW - timedelta(days=7), NOW, timedelta(hours=1), metadata, OP, 'hour', op_times)\n",
    "    get_time_based_meta(NOW - timedelta(weeks=4), NOW, timedelta(days=1), metadata, OP, 'day', op_times)\n",
    "    get_time_based_meta(NOW - timedelta(weeks=52), NOW, timedelta(weeks=4), metadata, OP, 'month', op_times)\n",
    "\n",
    "  # time buckets and FFT for 'all' operations\n",
    "  get_time_based_meta(NOW - timedelta(days=7), NOW, timedelta(hours=1), metadata, \"all_ops\", 'hour', sorted_ops)\n",
    "  get_time_based_meta(NOW - timedelta(weeks=4), NOW, timedelta(days=1), metadata, \"all_ops\", 'day', sorted_ops)\n",
    "  get_time_based_meta(NOW - timedelta(weeks=52), NOW, timedelta(weeks=4), metadata, \"all_ops\", 'month', sorted_ops)\n",
    "\n",
    "  # burstiness stats\n",
    "  get_burstiness(interarrivals, metadata)\n",
    "\n",
    "def clean_up_metadata(metadata):\n",
    "  # Clean up metadata we don't need\n",
    "  del metadata['locs']\n",
    "  del metadata['times']['all_ops']\n",
    "  for OP in UNIQUE_OPS:\n",
    "    if metadata['times'].get(OP):\n",
    "      del metadata['times'][OP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER: Extract meta features from audit log\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_user_info_from_auditlog(users, ra):\n",
    "  print(\"Iterate Audit Log\")\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "    uid = row_info['user_id']\n",
    "    user_info = {\n",
    "      \"file_infos\": [],\n",
    "      \"embedding\": []\n",
    "    }\n",
    "    if uid in users:\n",
    "      user_info: dict = users[uid]\n",
    "    \n",
    "    # Create metadata object for user if doesn't exist\n",
    "    metadata = user_info.setdefault(\"metadata\", {})\n",
    "    \n",
    "    setup_metadata_from_audit_log(metadata, row_info)\n",
    "\n",
    "\n",
    "def get_meta_features_users(users, num_top_locs=3, limit=1000000000):\n",
    "  ra = DFS['resource_auditrecord']\n",
    "  ra = ra.head(limit)\n",
    "\n",
    "  # Delete any existing metadata so we don't mix between runs\n",
    "  for user, user_info in users.items():\n",
    "    if user_info.get(\"metadata\"):\n",
    "      del user_info['metadata']\n",
    "\n",
    "  get_user_info_from_auditlog(users, ra)\n",
    "\n",
    "  print(\"Update Users Metadata\")\n",
    "  for user, user_info in tqdm(users.items(), total=len(users)):\n",
    "    if not user_info.get('metadata'):\n",
    "      # print(f\"No metadata for file {file}\")\n",
    "      continue\n",
    "    metadata = user_info['metadata']\n",
    "    if not metadata.get('locs'):\n",
    "      print(f\"No activity for user {user}\")\n",
    "      continue\n",
    "\n",
    "    fill_metadata_stats_for_file_or_users(metadata, [user])\n",
    "    clean_up_metadata(metadata)\n",
    "\n",
    "  return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE: Extract meta features from audit log\n",
    "from collections import Counter\n",
    "\n",
    "def get_file_info_from_auditlog(files, ra):\n",
    "  print(\"Iterate Audit Log\")\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "\n",
    "    audit_log_id = row_info['id']\n",
    "    if audit_log_id not in AUDIT_TO_FILE_MAPPING:\n",
    "      # print(\"No file info available\")\n",
    "      continue\n",
    "    file_hash = AUDIT_TO_FILE_MAPPING[row_info['id']]\n",
    "\n",
    "    if file_hash not in files:\n",
    "      # print(f\"Didn't find file hash in files... adding file\")\n",
    "      files[file_hash] = {}\n",
    "    file_info = files[file_hash]\n",
    "    \n",
    "    # Create metadata object for file if doesn't exist\n",
    "    metadata = file_info.setdefault(\"metadata\", {})\n",
    "\n",
    "    setup_metadata_from_audit_log(metadata, row_info)\n",
    "\n",
    "def get_meta_features_files(files, num_top_locs=3, limit=1000000000):\n",
    "  ra = DFS['resource_auditrecord']\n",
    "  ra = ra.head(limit)\n",
    "\n",
    "  # Delete any existing metadata so we don't mix between runs\n",
    "  for file, file_info in files.items():\n",
    "    if file_info.get(\"metadata\"):\n",
    "      del file_info['metadata']\n",
    "\n",
    "  get_file_info_from_auditlog(files, ra)\n",
    "\n",
    "  print(\"Update Files Metadata\")\n",
    "  for file, file_info in tqdm(files.items(), total=len(files)):\n",
    "    if not file_info.get('metadata'):\n",
    "      # print(f\"No metadata for file {file}\")\n",
    "      continue\n",
    "    metadata = file_info['metadata']\n",
    "    if not metadata.get('locs'):\n",
    "      # print(f\"No activity for file {file}\")\n",
    "      continue\n",
    "\n",
    "    users = [op['user'] for op in metadata['times']['all_ops']]\n",
    "    fill_metadata_stats_for_file_or_users(metadata, users)\n",
    "    clean_up_metadata(metadata)\n",
    "  return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10000000000000\n",
    "USERS = get_meta_features_users(USERS, limit=limit)\n",
    "FILES = get_meta_features_files(FILES, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_cache(AUDIT_TO_FILE_MAPPING, EMBEDDINGS_CACHE, FILES, USERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c36f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "ra = DFS['resource_auditrecord']\n",
    "rrn = DFS['resource_resourcenode']\n",
    "rr = DFS['resource_resource']\n",
    "rl = DFS['resource_label']\n",
    "rh = DFS['resource_hash']\n",
    "# rrn[rrn['id'] == 2444061]\n",
    "# rr[rr['id'] == 814033]\n",
    "# rr[rr['parent_id'] == 814033]\n",
    "# rl[rl['hash_id'] == 78048]\n",
    "# ra.head()\n",
    "# ra[ra['audited_id'] == 2444061.0]\n",
    "# len(rr['id'].unique())\n",
    "# rr[rr['resource_type'] == \"FILE\"]\n",
    "# rrn = dfs['resource_resourcenode']\n",
    "# len(rrn['resource_id'].unique())\n",
    "\n",
    "# Get valid audit record ids preemptively\n",
    "340 in set(AUDIT_TO_FILE_MAPPING.values())\n",
    "\n",
    "index = 4000\n",
    "# users_list = list(USERS.keys())\n",
    "# user_name = users_list[index]\n",
    "files_list = list(FILES.keys())\n",
    "file_hash = files_list[index]\n",
    "# USERS[user_name]['metadata']\n",
    "count = 0\n",
    "for hash, file in FILES.items():\n",
    "  if file.get(\"metadata\"):\n",
    "    count += 1\n",
    "\n",
    "print(count)\n",
    "# file_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5c98d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05757612",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095ef0d",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998f412",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

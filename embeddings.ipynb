{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb5b98b",
   "metadata": {},
   "source": [
    "# Representing Users and Files\n",
    "#### Design Document: https://docs.google.com/document/d/1F84Nj3IQ-f_36bmmsOTuOo9u65gfD1WU5LKdnw8ShcY/edit?tab=t.x97j5jy1kop1#heading=h.1vu1g9fe3ujo\n",
    "#### Optimizations:\n",
    "- Re-ranking topics\n",
    "- More information in file embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988742d",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af224c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install openai\n",
    "! pip install sentence-transformers\n",
    "! pip install torch\n",
    "! pip install faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a9bfb",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6653a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into pandas DataFrames\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_dir = \"./csvs\"\n",
    "\n",
    "dfs = {}\n",
    "for filename in os.listdir(csv_dir):\n",
    "    path = csv_dir + \"/\" + filename\n",
    "    try:\n",
    "        name_no_ext = filename.split('.')[0]\n",
    "        dfs[name_no_ext] = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eae9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print brief summary and show first few rows for each loaded dataframe\n",
    "from IPython.display import display\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb592e66",
   "metadata": {},
   "source": [
    "# Topic Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'BAAI/bge-large-zh-v1.5'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5) Cache any embeddings we calculate. Keep in seperate cell for safety\n",
    "embeddings_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) For each file, get its topics. Create dict of {file_hash_id: {labels: [{label_name: str, label_embedding: [int]}], embedding: int}]\n",
    "# 1.1) Create topic embedding with embedding model, store in cache\n",
    "# 1.2) Average those embeddings to get the file representation\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def average_embeddings(embeddings):\n",
    "  return np.mean(embeddings, axis=0)\n",
    "\n",
    "def get_embedding_for_text(text: str):\n",
    "  text = text.lower()\n",
    "\n",
    "  if text in embeddings_cache:\n",
    "    return embeddings_cache[text]\n",
    "  \n",
    "  embeddings_cache[text] = model.encode(text, normalize_embeddings=True) # normalize for cosine similarity\n",
    "  return embeddings_cache[text]\n",
    "\n",
    "def get_file_embeddings(limit=None):\n",
    "  files = {}\n",
    "  rl = dfs['resource_label']\n",
    "  rl = rl[rl['name'] == \"topic\"]\n",
    "\n",
    "  print(\"Number of resources to work through: \", len(rl))\n",
    "  for row in tqdm(rl.iterrows()):\n",
    "\n",
    "    # Break out early if we want\n",
    "    if limit is not None:\n",
    "      limit -= 1\n",
    "      if limit <= 0:\n",
    "        break\n",
    "\n",
    "    resource_info = row[1]\n",
    "    file_info = files.setdefault(resource_info['hash_id'], {\n",
    "      \"labels\": [],\n",
    "      \"embedding\": []\n",
    "    })\n",
    "\n",
    "    if (resource_info[\"name\"] != \"topic\"):\n",
    "      continue\n",
    "\n",
    "    file_info[\"labels\"].append({\n",
    "      \"id\": resource_info[\"id\"],\n",
    "      \"name\": resource_info[\"value\"]\n",
    "    })\n",
    "\n",
    "  print(\"Number of files to work through\")\n",
    "  for _, info in tqdm(files.items()):\n",
    "    info['embedding'] = average_embeddings([get_embedding_for_text(label['name']) for label in info['labels']])\n",
    "  return files\n",
    "\n",
    "files = get_file_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7648a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3) Store our results\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def store_results():\n",
    "  with open(\"pickle/topic_embeddings_bge-large-zh-v1.5\", \"wb\") as f:\n",
    "    pickle.dump(embeddings_cache, f)\n",
    "  with open(\"pickle/file_embeddings_\" + str(datetime.now()), \"wb\") as f:\n",
    "    pickle.dump(files, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = dfs['resource_auditrecord']\n",
    "rrn = dfs['resource_resourcenode']\n",
    "rr = dfs['resource_resource']\n",
    "rl = dfs['resource_label']\n",
    "# rrn[rrn['id'] == 2444061]\n",
    "# rr[rr['id'] == 814033]\n",
    "rl[rl['hash_id'] == 78048]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) For each user, time-weight the relevant file-embeddings. Create dict of {user_id: embedding}\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_stream(audit_id, file_id, streams, timestamp, operation):\n",
    "    # File has been changed in some way, we want to find the first STREAM that is after the audit log timestamp\n",
    "    should_check_after = operation in ['MODIFIED', 'FILE_UPLOADED', 'RENAMED']\n",
    "    if len(streams) == 0:\n",
    "      raise ValueError(f\"No streams for audit id: {audit_id}, file id: {file_id} \")\n",
    "    \n",
    "    # 1 stream, always pick it\n",
    "    if len(streams) == 1:\n",
    "      stream = streams.iloc[0]\n",
    "    else:\n",
    "      streams = streams.sort_values(by=\"timestamp\")\n",
    "      if should_check_after:\n",
    "        filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "        if len(filtered_streams) == 0:\n",
    "          # Fallback to filtering opposite way\n",
    "          filtered_streams = streams[streams['timestamp'] <= timestamp]\n",
    "          stream = filtered_streams.iloc[-1]\n",
    "        else:\n",
    "          stream = filtered_streams.iloc[0]\n",
    "      else:\n",
    "        filtered_streams = streams[streams['timestamp'] <= timestamp]\n",
    "        if len(filtered_streams) == 0:\n",
    "          # Fallback to filtering opposite way\n",
    "          filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "          stream = filtered_streams.iloc[0]\n",
    "        else:\n",
    "          stream = filtered_streams.iloc[-1]\n",
    "    return stream\n",
    "\n",
    "def get_file_hash_as_of_audit(audit_id, operation, timestamp):\n",
    "  rrn, rr = dfs['resource_resourcenode'], dfs['resource_resource']\n",
    "  resource_node = rrn[rrn['id'] == audit_id]\n",
    "  resource_id = resource_node['resource_id']\n",
    "\n",
    "  # Should only be one...\n",
    "  if len(resource_id) != 1:\n",
    "      raise ValueError(f\"Found more than one resource with ID: {resource_id}. Audit id: {audit_id}\")\n",
    "  resource_id = resource_id.iloc[0]  # Use iloc[0] instead of .item()\n",
    "\n",
    "  # Grab the resource reference by the audit log event\n",
    "  resource = rr[rr['id'] == resource_id]\n",
    "  if len(resource) != 1:\n",
    "      raise ValueError(f\"Resource length greater than 1: {resource}. Audit id: {audit_id}\")\n",
    "\n",
    "  # Work our way down to the relevant STREAM, or in other words actual data, relevant for this audit log event\n",
    "  file_or_stream = resource.iloc[0]  # Use iloc[0] instead of .item()\n",
    "\n",
    "  if file_or_stream['resource_type'] != \"STREAM\":\n",
    "      # Must be a file\n",
    "      assert file_or_stream['resource_type'] == \"FILE\"\n",
    "      file = file_or_stream\n",
    "      streams = rr[rr['parent_id'] == file['id']]\n",
    "      stream = get_stream(audit_id, file['id'], streams, timestamp, operation)\n",
    "      if stream is None:\n",
    "          return None\n",
    "      hash_id = stream['hash_id']\n",
    "  else:\n",
    "      hash_id = file_or_stream['hash_id']\n",
    "\n",
    "  return hash_id\n",
    "\n",
    "def get_user_embedding(user, tau=60*60*24*30): \n",
    "  '''\n",
    "  1) Default tau to 1 month, in other words, we lose 37% of old information if \n",
    "     a month has passed since last indexing. Exponential decay.\n",
    "  2) Unit of time is seconds\n",
    "  '''\n",
    "  assert len(user['file_infos']) > 0\n",
    "\n",
    "  user_embedding = np.array(user['file_infos'][0]['embedding'])\n",
    "  embedding_time = user['file_infos'][0]['timestamp']\n",
    "\n",
    "  # Build up the index recursively\n",
    "  for file_info in user['file_infos'][1:]:\n",
    "    # Get difference from last time and update time variable\n",
    "    d_t = datetime.fromisoformat(file_info['timestamp']) - datetime.fromisoformat(embedding_time)\n",
    "    d_t = d_t.total_seconds()\n",
    "    embedding_time = file_info['timestamp']\n",
    "\n",
    "    assert d_t > 0, \"User file access times must be ordered\"\n",
    "\n",
    "    # Apply the weighting\n",
    "    # d_t and tau should both be in seconds\n",
    "    alpha = 1 - math.exp(-d_t / tau)\n",
    "    user_embedding = alpha * np.array(file_info['embedding']) + (1 - alpha) * user_embedding\n",
    "\n",
    "  return user_embedding\n",
    "\n",
    "def get_user_embeddings():\n",
    "  users = {}\n",
    "  ar = dfs['resource_auditrecord']\n",
    "  print(\"Processing audit log records:\")\n",
    "  for row in tqdm(ar.iterrows(), total=len(ar)):\n",
    "    row_info = row[1]\n",
    "    user_info = users.setdefault(row_info['user_id'], {\n",
    "      \"file_infos\": []\n",
    "    })\n",
    "    time_of_operation = row_info[\"timestamp\"]\n",
    "    try:\n",
    "      hash_id = get_file_hash_as_of_audit(row_info['audited_id'], row_info['operation'], time_of_operation)\n",
    "    except Exception as e:\n",
    "      print(f\"Exception caught: {e}\")\n",
    "      continue\n",
    "\n",
    "    if hash_id not in files:\n",
    "      print(f\"Didn't find {hash_id} in file, must have failed topic extraction\")\n",
    "      continue\n",
    "    file_info = files[hash_id]\n",
    "    file_embedding = file_info[\"embedding\"]\n",
    "    user_info['file_infos'].append({\n",
    "      \"timestamp\": time_of_operation,\n",
    "      \"embedding\": file_embedding\n",
    "    })\n",
    "\n",
    "  # 2.1) Time weight the embeddings\n",
    "  print(\"Getting user embeddings: \")\n",
    "  for user, user_info in tqdm(users.items()):\n",
    "    user_info[\"embedding\"] = get_user_embedding(user_info)\n",
    "  return users\n",
    "\n",
    "# users = get_user_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c821f907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.36787944, 0.36787944, 0.36787944])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_1 = {'file_infos': [{'timestamp': '2025-09-28 01:29:31.77+00', 'embedding': [ 1, 1, 1 ]}, {'timestamp': '2025-10-28 01:29:31.77+00', 'embedding': [ 0, 0, 0 ]}]}\n",
    "get_user_embedding(user_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fc145",
   "metadata": {},
   "source": [
    "# Meta Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151daa08",
   "metadata": {},
   "source": [
    "# User/File Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095ef0d",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5c98d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05757612",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998f412",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

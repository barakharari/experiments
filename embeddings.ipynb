{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb5b98b",
   "metadata": {},
   "source": [
    "# Representing Users and Files\n",
    "#### Design Document: https://docs.google.com/document/d/1F84Nj3IQ-f_36bmmsOTuOo9u65gfD1WU5LKdnw8ShcY/edit?tab=t.x97j5jy1kop1#heading=h.1vu1g9fe3ujo\n",
    "#### Optimizations:\n",
    "- Re-ranking topics\n",
    "- More information in file embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988742d",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af224c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install openai\n",
    "! pip install sentence-transformers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a9bfb",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6653a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files into pandas DataFrames\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_dir = \"./csvs\"\n",
    "\n",
    "dfs = {}\n",
    "for filename in os.listdir(csv_dir):\n",
    "    path = csv_dir + \"/\" + filename\n",
    "    try:\n",
    "        name_no_ext = filename.split('.')[0]\n",
    "        dfs[name_no_ext] = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26eae9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print brief summary and show first few rows for each loaded dataframe\n",
    "from IPython.display import display\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb592e66",
   "metadata": {},
   "source": [
    "# User/File Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'BAAI/bge-large-zh-v1.5'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5) Cache any embeddings we calculate.\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "cache_dir = \"./pickle/\"\n",
    "\n",
    "def _most_recent_file(pattern):\n",
    "  # Return the most recently modified file in cache_dir matching the glob pattern, or None if none found\n",
    "  matches = glob.glob(os.path.join(cache_dir, pattern))\n",
    "  if not matches:\n",
    "    return None\n",
    "  return max(matches, key=os.path.getmtime)\n",
    "\n",
    "def get_cache():\n",
    "  audit_to_file_mapping_file_path = _most_recent_file(\"audit_to_file_mapping_*.pkl\")\n",
    "  topic_embeddings_file_path = _most_recent_file(\"topic_embeddings_bge-large-zh-v1.5_*.pkl\")\n",
    "  file_embeddings_file_path = _most_recent_file(\"file_embeddings_*.pkl\")\n",
    "  user_embeddings_file_path = _most_recent_file(\"user_embeddings_*.pkl\")\n",
    "\n",
    "  audit_to_file_mapping, embeddings_cache, files, users = {}, {}, {}, {}\n",
    "\n",
    "  if audit_to_file_mapping_file_path and os.path.exists(audit_to_file_mapping_file_path):\n",
    "    with open(audit_to_file_mapping_file_path, \"rb\") as p:\n",
    "      audit_to_file_mapping = pickle.load(p)\n",
    "\n",
    "  if topic_embeddings_file_path and os.path.exists(topic_embeddings_file_path):\n",
    "    with open(topic_embeddings_file_path, \"rb\") as p:\n",
    "      embeddings_cache = pickle.load(p)\n",
    "\n",
    "  if file_embeddings_file_path and os.path.exists(file_embeddings_file_path):\n",
    "    with open(file_embeddings_file_path, \"rb\") as p:\n",
    "      files = pickle.load(p)\n",
    "\n",
    "  if user_embeddings_file_path and os.path.exists(user_embeddings_file_path):\n",
    "    with open(user_embeddings_file_path, \"rb\") as p:\n",
    "      users = pickle.load(p)\n",
    "\n",
    "  print(f\"Audit to file mapping size: {len(audit_to_file_mapping)}\")\n",
    "  print(f\"Topic embeddings cache size: {len(embeddings_cache) if embeddings_cache is not None else 0}\")\n",
    "  print(f\"Files size: {len(files) if files is not None else 0}\")\n",
    "  print(f\"Users size: {len(users) if users is not None else 0}\")\n",
    "\n",
    "  return audit_to_file_mapping, embeddings_cache, files, users\n",
    "\n",
    "\n",
    "def save_to_cache(audit_to_file_mapping, embeddings_cache, files, users):\n",
    "  # Ensure cache dir exists\n",
    "  os.makedirs(cache_dir, exist_ok=True)\n",
    "  NOW = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "  with open(os.path.join(cache_dir, f\"topic_embeddings_bge-large-zh-v1.5_{now}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(embeddings_cache, f)\n",
    "  with open(os.path.join(cache_dir, f\"file_embeddings_{now}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(files, f)\n",
    "  with open(os.path.join(cache_dir, f\"user_embeddings_{now}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(users, f)\n",
    "  with open(os.path.join(cache_dir, f\"audit_to_file_mapping_{now}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(audit_to_file_mapping, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243c5df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "files[81523]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) For each file, get its topics. Create dict of {file_hash_id: {labels: [{label_name: str, label_embedding: [int]}], embedding: int}]\n",
    "# 1.1) Create topic embedding with embedding model, store in cache\n",
    "# 1.2) Average those embeddings to get the file representation\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def average_embeddings(embeddings):\n",
    "  return np.mean(embeddings, axis=0)\n",
    "\n",
    "def get_embedding_for_text(text: str):\n",
    "  text = text.lower()\n",
    "\n",
    "  if text in embeddings_cache:\n",
    "    return embeddings_cache[text]\n",
    "  \n",
    "  embeddings_cache[text] = model.encode(text, normalize_embeddings=True) # normalize for cosine similarity\n",
    "  return embeddings_cache[text]\n",
    "\n",
    "def get_file_embeddings(limit=None):\n",
    "  files = {}\n",
    "  rl = dfs['resource_label']\n",
    "  rl = rl[rl['name'] == \"topic\"]\n",
    "\n",
    "  print(\"Number of resources to work through: \", len(rl))\n",
    "  for row in tqdm(rl.iterrows()):\n",
    "\n",
    "    # Break out early if we want\n",
    "    if limit is not None:\n",
    "      limit -= 1\n",
    "      if limit <= 0:\n",
    "        break\n",
    "\n",
    "    resource_info = row[1]\n",
    "    file_info = files.setdefault(resource_info['hash_id'], {\n",
    "      \"labels\": [],\n",
    "      \"embedding\": []\n",
    "    })\n",
    "\n",
    "    if (resource_info[\"name\"] != \"topic\"):\n",
    "      continue\n",
    "\n",
    "    file_info[\"labels\"].append({\n",
    "      \"id\": resource_info[\"id\"],\n",
    "      \"name\": resource_info[\"value\"]\n",
    "    })\n",
    "\n",
    "  print(\"Number of files to work through\")\n",
    "  for _, info in tqdm(files.items()):\n",
    "    info['embedding'] = average_embeddings([get_embedding_for_text(label['name']) for label in info['labels']])\n",
    "  return files\n",
    "\n",
    "# Didn't load from cache\n",
    "if len(files) == 0:\n",
    "  files = get_file_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f2c8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get valid audit ids\n",
    "\n",
    "from functools import cache\n",
    "\n",
    "@cache\n",
    "def get_valid_audit_ids():\n",
    "  rr, rrn, ra = dfs[\"resource_resource\"], dfs[\"resource_resourcenode\"], dfs[\"resource_auditrecord\"]\n",
    "  valid_resource_ids = set()\n",
    "  print(\"Get valid resources\")\n",
    "  for row in tqdm(rr.iterrows(), total=len(rr)):\n",
    "    row_info = row[1]\n",
    "    id = row_info['id']\n",
    "\n",
    "    if len(rr[rr['parent_id'] == id]) != 0:\n",
    "      valid_resource_ids.add(id)\n",
    "\n",
    "  valid_resource_node_ids = set()\n",
    "  print(\"Get valid resource nodes\")\n",
    "  for row in tqdm(rrn.iterrows(), total=len(rrn)):\n",
    "    row_info = row[1]\n",
    "    if row_info['resource_id'] in valid_resource_ids:\n",
    "      valid_resource_node_ids.add(row_info['id'])\n",
    "\n",
    "  valid_audit_ids = set()\n",
    "  ra.head()\n",
    "  print(\"Get valid audit ids\")\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "    if row_info['audited_id'] in valid_resource_node_ids:\n",
    "      valid_audit_ids.add(row_info['id'])\n",
    "  return valid_audit_ids, valid_resource_node_ids, valid_resource_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) For each user, time-weight the relevant file-embeddings. Create dict of {user_id: embedding}\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_stream(audit_id, file_id, streams, timestamp, operation):\n",
    "    # File has been changed in some way, we want to find the first STREAM that is after the audit log timestamp\n",
    "    should_check_after = operation in ['MODIFIED', 'FILE_UPLOADED', 'RENAMED']\n",
    "    if len(streams) == 0:\n",
    "      raise ValueError(f\"No streams for audit id: {audit_id}, file id: {file_id} \")\n",
    "    \n",
    "    # 1 stream, always pick it\n",
    "    if len(streams) == 1:\n",
    "      stream = streams.iloc[0]\n",
    "    else:\n",
    "      streams = streams.sort_values(by=\"timestamp\")\n",
    "      if should_check_after:\n",
    "        filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "        if len(filtered_streams) == 0:\n",
    "          # Fallback to filtering opposite way\n",
    "          filtered_streams = streams[streams['timestamp'] <= timestamp]\n",
    "          stream = filtered_streams.iloc[-1]\n",
    "        else:\n",
    "          stream = filtered_streams.iloc[0]\n",
    "      else:\n",
    "        filtered_streams = streams[streams['timestamp'] <= timestamp]\n",
    "        if len(filtered_streams) == 0:\n",
    "          # Fallback to filtering opposite way\n",
    "          filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "          stream = filtered_streams.iloc[0]\n",
    "        else:\n",
    "          stream = filtered_streams.iloc[-1]\n",
    "    return stream\n",
    "\n",
    "def get_file_hash_as_of_audit(audit_id, operation, timestamp):\n",
    "  rrn, rr = dfs['resource_resourcenode'], dfs['resource_resource']\n",
    "  resource_node = rrn[rrn['id'] == audit_id]\n",
    "  resource_id = resource_node['resource_id']\n",
    "\n",
    "  # Assume only one resource id associated with audit id?\n",
    "  if len(resource_id) != 1:\n",
    "      raise ValueError(f\"Found more than one resource with ID: {resource_id}. Audit id: {audit_id}\")\n",
    "  resource_id = resource_id.iloc[0]\n",
    "\n",
    "  # Grab the resource reference by the audit log event\n",
    "  resource = rr[rr['id'] == resource_id]\n",
    "  if len(resource) != 1:\n",
    "      raise ValueError(f\"Resource length greater than 1: {resource}. Audit id: {audit_id}\")\n",
    "\n",
    "  # Work our way down to the relevant STREAM, or in other words actual data, relevant for this audit log event\n",
    "  file_or_stream = resource.iloc[0]\n",
    "\n",
    "  if file_or_stream['resource_type'] != \"STREAM\":\n",
    "      # Must be a file\n",
    "      assert file_or_stream['resource_type'] == \"FILE\"\n",
    "      file = file_or_stream\n",
    "      streams = rr[rr['parent_id'] == file['id']]\n",
    "      stream = get_stream(audit_id, file['id'], streams, timestamp, operation)\n",
    "      if stream is None:\n",
    "          # Found no stream for file id\n",
    "          raise ValueError(f\"Found no stream for audit id: {audit_id}, file_id: {file['id']}\")\n",
    "      hash_id = stream['hash_id']\n",
    "  else:\n",
    "      # If stream, directly get hash id\n",
    "      hash_id = file_or_stream['hash_id']\n",
    "\n",
    "  return hash_id\n",
    "\n",
    "# Cache mapping from audit log record to file\n",
    "audit_id_to_file_hash = {}\n",
    "\n",
    "def aggregate_file_embeddings_per_user(valid_audit_ids=None):\n",
    "  limit = 10000000000\n",
    "  users = {}\n",
    "  ar = dfs['resource_auditrecord']\n",
    "\n",
    "  # Only consider audit ids that point to actual files when dealing with topic embeddings for user\n",
    "  if not valid_audit_ids:\n",
    "    valid_audit_ids = get_valid_audit_ids()\n",
    "\n",
    "  ar = ar[ar['id'].isin(valid_audit_ids)]\n",
    "  \n",
    "  bad_resource_ids = set()\n",
    "  good_resource_ids = set()\n",
    "\n",
    "  print(\"Processing audit log records:\")\n",
    "  for row in tqdm(ar.iterrows(), total=len(ar)):\n",
    "    \n",
    "    row_info = row[1]\n",
    "    if row_info['audited_id'] in bad_resource_ids:\n",
    "      continue\n",
    "    \n",
    "    if len(bad_resource_ids) > limit:\n",
    "      break\n",
    "    \n",
    "    user_info = users.setdefault(row_info['user_id'], {\n",
    "      \"file_infos\": []\n",
    "    })\n",
    "    time_of_operation = row_info[\"timestamp\"]\n",
    "    try:\n",
    "      # Get most relevant file version as of audit time\n",
    "      hash_id = get_file_hash_as_of_audit(row_info['audited_id'], row_info['operation'], time_of_operation)\n",
    "    except Exception as e:\n",
    "      # print(f\"Exception caught: {e}\")\n",
    "      bad_resource_ids.add(row_info['audited_id'])\n",
    "      continue\n",
    "\n",
    "    if hash_id not in files:\n",
    "      # print(f\"Didn't find {hash_id} in file, audit id: {row_info['audited_id']}, must have failed topic extraction\")\n",
    "      continue\n",
    "\n",
    "    # Cache audit id to file mapping for later\n",
    "    audit_id_to_file_hash[row_info['id']] = hash_id\n",
    "\n",
    "    good_resource_ids.add(row_info['audited_id'])\n",
    "\n",
    "    # If successfully got hash, lookup in files table and add to user info\n",
    "    file_info = files[hash_id]\n",
    "    file_embedding = file_info[\"embedding\"]\n",
    "    user_info['file_infos'].append({\n",
    "      \"timestamp\": datetime.fromisoformat(time_of_operation),\n",
    "      \"embedding\": file_embedding\n",
    "    })\n",
    "\n",
    "  # print(f\"Success: {len(good_resource_ids)}, Error: {len(bad_resource_ids)}\")\n",
    "  return users\n",
    "\n",
    "valid_audit_ids, _, _ = get_valid_audit_ids()\n",
    "users = aggregate_file_embeddings_per_user(valid_audit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e553de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ra['id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61972b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_user_embedding(uid, user, tau=60*60*24*30): \n",
    "  '''\n",
    "  1) Default tau to 1 month, in other words, we lose 37% of old information if \n",
    "     a month has passed since last indexing. Exponential decay.\n",
    "  2) Unit of time is seconds\n",
    "  '''\n",
    "  assert len(user['file_infos']) > 0, f\"User has no files {uid}\"\n",
    "\n",
    "  user_embedding = np.array(user['file_infos'][0]['embedding'])\n",
    "  embedding_time = user['file_infos'][0]['timestamp']\n",
    "\n",
    "  # Build up the index recursively\n",
    "  for file_info in user['file_infos'][1:]:\n",
    "    # Get difference from last time and update time variable\n",
    "    d_t = datetime.fromisoformat(file_info['timestamp']) - datetime.fromisoformat(embedding_time)\n",
    "    d_t = d_t.total_seconds()\n",
    "    embedding_time = file_info['timestamp']\n",
    "\n",
    "    assert d_t >= 0, f\"User file access times must be ordered: from - {embedding_time}, to - {file_info['timestamp']}\"\n",
    "\n",
    "    # Apply the weighting\n",
    "    # d_t and tau should both be in seconds\n",
    "    alpha = 1 - math.exp(-d_t / tau)\n",
    "    user_embedding = alpha * np.array(file_info['embedding']) + (1 - alpha) * user_embedding\n",
    "\n",
    "  return user_embedding\n",
    "\n",
    "def get_user_embeddings(users):\n",
    "  # 2.1) Time weight the file embeddings\n",
    "  print(\"Calculating user embeddings: \")\n",
    "  for user, user_info in tqdm(users.items()):\n",
    "    user_info['file_infos'] = sorted(user_info['file_infos'], key=lambda event: event['timestamp'])\n",
    "    try:\n",
    "      user_info[\"embedding\"] = get_user_embedding(user, user_info)\n",
    "    except Exception as e:\n",
    "      print(f\"Caught exception with error: {e}\")\n",
    "  return users\n",
    "\n",
    "users = get_user_embeddings(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_cache(audit_id_to_file_hash, embeddings_cache, files, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821f907",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_file_hash_as_of_audit(2444061, \"FILE_DOWNLOADED\", '2025-09-27 23:13:29.745+00')\n",
    "# users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ae30f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "ra = dfs['resource_auditrecord']\n",
    "rrn = dfs['resource_resourcenode']\n",
    "rr = dfs['resource_resource']\n",
    "rl = dfs['resource_label']\n",
    "# rrn[rrn['id'] == 2444061]\n",
    "# rr[rr['id'] == 814033]\n",
    "# rr[rr['parent_id'] == 814033]\n",
    "# rl[rl['hash_id'] == 78048]\n",
    "# ra.head()\n",
    "# ra[ra['audited_id'] == 2444061.0]\n",
    "# len(rr['id'].unique())\n",
    "# rr[rr['resource_type'] == \"FILE\"]\n",
    "# rrn = dfs['resource_resourcenode']\n",
    "# len(rrn['resource_id'].unique())\n",
    "\n",
    "# Get valid audit record ids preemptively\n",
    "ra.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fc145",
   "metadata": {},
   "source": [
    "# Meta Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_to_file_mapping, embeddings_cache, files, users  = get_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "cc56ac44",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNIQUE_OPS = ra['operation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fafb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "NOW = datetime.now(timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER: Extract meta features from audit log\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import math\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def get_interarrivals(file_infos):\n",
    "  interarrivals = []\n",
    "  if len(file_infos) <= 1:\n",
    "    return []\n",
    "  start_time = file_infos[0][\"timestamp\"]\n",
    "  for fi in file_infos[1:]:\n",
    "    next_time = fi['timestamp']\n",
    "    diff = (next_time - start_time).total_seconds()\n",
    "    assert diff >= 0, f\"not ordered: start_time: {start_time}, next_time: {next_time}\"\n",
    "    interarrivals.append(diff)\n",
    "    start_time = next_time\n",
    "  return interarrivals\n",
    "\n",
    "def get_sit(interarrivals, mean):\n",
    "  deviations = [math.pow(i - mean, 2) for i in interarrivals]\n",
    "  if len(deviations) == 0:\n",
    "    return -1\n",
    "  return math.sqrt(sum(deviations) / len(deviations))\n",
    "\n",
    "def add_meta_to_user_info(user_info):\n",
    "    # Sort file operations by time if not sorted already\n",
    "    user_info['file_infos'] = sorted(user_info['file_infos'], key=lambda file_info: file_info['timestamp'])\n",
    "\n",
    "    metadata = user_info.setdefault(\"metadata\", {})\n",
    "    # mean interarrival time\n",
    "    interarrivals: list = get_interarrivals(user_info['file_infos'])\n",
    "    metadata['mit'] = sum(interarrivals) / len(interarrivals) if len(interarrivals) > 0 else -1\n",
    "    # std interarrival time\n",
    "    metadata['sit'] = get_sit(interarrivals, metadata['mit'])\n",
    "    # recency\n",
    "    # Should be ordered by time\n",
    "    if len(user_info['file_infos']) == 0:\n",
    "      metadata['recency'] = -1\n",
    "    else:\n",
    "      metadata['recency'] = (NOW - user_info['file_infos'][-1]['timestamp']).total_seconds()\n",
    "    # user distribution entropy\n",
    "    metadata['ud_entropy'] = 0\n",
    "    # unique users\n",
    "    metadata['unique_users'] = 1\n",
    "    # top users\n",
    "    metadata['top_users'] = 1  \n",
    "\n",
    "def get_user_info_from_auditlog(users, ra):\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "    uid = row_info['user_id']\n",
    "    user_info = {\n",
    "      \"file_infos\": [],\n",
    "      \"embedding\": []\n",
    "    }\n",
    "    if uid in users:\n",
    "      user_info: dict = users[uid]\n",
    "    \n",
    "    # Create metadata object for user if doesn't exist\n",
    "    metadata = user_info.setdefault(\"metadata\", {})\n",
    "    \n",
    "    # Add location information\n",
    "    locs = metadata.setdefault('locs', list())\n",
    "    locs.append(f'{row_info['geolocation']}-{row_info['client_ip']}')\n",
    "\n",
    "    # Add timestamps per operations\n",
    "    operation = row_info['operation']\n",
    "    times = metadata.setdefault('times', {})\n",
    "    operation_times = times.setdefault(operation, [])\n",
    "    operation_times.append(datetime.fromisoformat(row_info['timestamp']))\n",
    "\n",
    "def get_fft_info(buckets):\n",
    "  # Calculate top 3 frequencies/amplitudes/phases of time series using FFT\n",
    "  bucket_arr = np.array(buckets)\n",
    "  fft_result = np.fft.fft(bucket_arr)\n",
    "  fft_freqs = np.fft.fftfreq(len(bucket_arr), d=1)\n",
    "  amplitudes = np.abs(fft_result)\n",
    "  phases = np.angle(fft_result)\n",
    "  # Ignore the zero frequency (DC component)\n",
    "  indices = np.argsort(amplitudes[1:])[::-1][:3] + 1 if len(amplitudes) > 1 else []\n",
    "  return [(fft_freqs[i], amplitudes[i], phases[i]) for rank, i in enumerate(indices)]\n",
    "\n",
    "def get_time_based_meta(start, end, interval, meta, operation, time_str):\n",
    "  # Sort datetimes so it's O(n) operation to create buckets\n",
    "  times = sorted(meta['times'][operation])\n",
    "  buckets = []\n",
    "  current = start\n",
    "  idx = 0\n",
    "  n = len(times)\n",
    "  while current < end:\n",
    "      next_bucket = current + interval\n",
    "      count = 0\n",
    "      # Count how many times fall into [current, next_bucket)\n",
    "      while idx < n and times[idx] < next_bucket:\n",
    "          if times[idx] >= current:\n",
    "              count += 1\n",
    "          idx += 1\n",
    "      buckets.append(count)\n",
    "      current = next_bucket\n",
    "  \n",
    "  for i, bucket in enumerate(buckets):\n",
    "    meta['times'][f'{operation}_{time_str}_{i}'] = bucket\n",
    "  fft_infos = get_fft_info(buckets)\n",
    "  for i, info in enumerate(fft_infos):\n",
    "    meta['times'][f'{operation}_{time_str}_freq_{i}'] = info[0]\n",
    "    meta['times'][f'{operation}_{time_str}_amp_{i}'] = info[1]\n",
    "    meta['times'][f'{operation}_{time_str}_phase_{i}'] = info[2]\n",
    "\n",
    "def get_burstiness(interarrivals, meta):\n",
    "  # TODO\n",
    "  pass\n",
    "\n",
    "\n",
    "def get_meta_features_users(users, num_top_locs=3):\n",
    "  ra = dfs['resource_auditrecord']\n",
    "  # ra = ra.head(10000)\n",
    "\n",
    "  for user, user_info in users.items():\n",
    "    if user_info.get(\"metadata\"):\n",
    "      del user_info['metadata']\n",
    "\n",
    "  print(\"Iterate Audit Log\")\n",
    "  get_user_info_from_auditlog(users, ra)\n",
    "\n",
    "  print(\"Update Users Metadata\")\n",
    "  for user, user_info in tqdm(users.items(), total=len(users)):\n",
    "    add_meta_to_user_info(user_info)\n",
    "\n",
    "    meta = user_info['metadata']\n",
    "    if not meta.get('locs'):\n",
    "      print(f\"No locs for user {user}\")\n",
    "      continue\n",
    "\n",
    "    # unique locations\n",
    "    meta['unique_locs'] = len(set(meta['locs']))\n",
    "\n",
    "    # top locations\n",
    "    counter = Counter(meta['locs'])\n",
    "    top_locs = counter.most_common(num_top_locs)\n",
    "    for i, loc in enumerate(top_locs):\n",
    "      meta[f'top_loc_{i}'] = loc[0]\n",
    "      meta[f'top_loc_{i}_per'] = loc[1] / len(meta['locs'])\n",
    "\n",
    "    # time meta\n",
    "    for OP in UNIQUE_OPS:\n",
    "      if not meta.get(\"times\"):\n",
    "        break\n",
    "      times = meta[\"times\"]\n",
    "      if not times.get(OP):\n",
    "        times[OP] = []\n",
    "      \n",
    "      # time buckets and FFT\n",
    "      get_time_based_meta(NOW - timedelta(days=7), NOW, timedelta(hours=1), meta, OP, 'hour')\n",
    "      get_time_based_meta(NOW - timedelta(weeks=4), NOW, timedelta(days=1), meta, OP, 'day')\n",
    "      get_time_based_meta(NOW - timedelta(weeks=52), NOW, timedelta(weeks=4), meta, OP, 'month')\n",
    "\n",
    "    get_burstiness(get_interarrivals(user_info['file_infos']), meta)\n",
    "\n",
    "    # Clean up metadata we don't need\n",
    "    del meta['locs']\n",
    "    for OP in UNIQUE_OPS:\n",
    "      del meta['times'][OP]\n",
    "\n",
    "  return users\n",
    "\n",
    "users = get_meta_features_users(users) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6e5ab28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>seqnum</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>params</th>\n",
       "      <th>name</th>\n",
       "      <th>record_id</th>\n",
       "      <th>operation</th>\n",
       "      <th>geolocation</th>\n",
       "      <th>client_ip</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_agent</th>\n",
       "      <th>batch_id</th>\n",
       "      <th>auditor_id</th>\n",
       "      <th>audited_id</th>\n",
       "      <th>application_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16225255</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-28 01:29:31.77+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QBR - Q4'2025- Cloud Team Template - Farmer Ga...</td>\n",
       "      <td>6863310073061781342</td>\n",
       "      <td>FILE_DOWNLOADED</td>\n",
       "      <td>US-VA</td>\n",
       "      <td>172.190.156.130</td>\n",
       "      <td>gabib@terasky.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>813555</td>\n",
       "      <td>2443907.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16227582</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-27 23:13:29.745+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TeraSky Baltic Rebilling - Customer List</td>\n",
       "      <td>2798679710274239636</td>\n",
       "      <td>FILE_DOWNLOADED</td>\n",
       "      <td>US-VA</td>\n",
       "      <td>172.190.156.130</td>\n",
       "      <td>gediminas@terasky.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>813555</td>\n",
       "      <td>2444061.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16229912</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-27 21:31:40.133+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VCF Migration Plan for Existing vSphere Enviro...</td>\n",
       "      <td>2454216044330707803</td>\n",
       "      <td>FILE_DOWNLOADED</td>\n",
       "      <td>US-VA</td>\n",
       "      <td>172.190.156.130</td>\n",
       "      <td>mandeep@terasky.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>813555</td>\n",
       "      <td>2444217.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16230034</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-27 21:31:10.928+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VCF Migration Plan for Existing vSphere Enviro...</td>\n",
       "      <td>-4490931525388484148</td>\n",
       "      <td>FILE_DOWNLOADED</td>\n",
       "      <td>US-VA</td>\n",
       "      <td>172.190.156.130</td>\n",
       "      <td>mandeep@terasky.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>813555</td>\n",
       "      <td>2444217.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16222754</td>\n",
       "      <td>3</td>\n",
       "      <td>2025-09-28 04:19:24.837+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vinted License Count</td>\n",
       "      <td>8789196147665239826</td>\n",
       "      <td>READ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>google-workspace-api@vinted-it.iam.gserviceacc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>813555</td>\n",
       "      <td>2443585.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  seqnum                   timestamp params  \\\n",
       "0  16225255       3   2025-09-28 01:29:31.77+00    NaN   \n",
       "1  16227582       3  2025-09-27 23:13:29.745+00    NaN   \n",
       "2  16229912       3  2025-09-27 21:31:40.133+00    NaN   \n",
       "3  16230034       3  2025-09-27 21:31:10.928+00    NaN   \n",
       "4  16222754       3  2025-09-28 04:19:24.837+00    NaN   \n",
       "\n",
       "                                                name            record_id  \\\n",
       "0  QBR - Q4'2025- Cloud Team Template - Farmer Ga...  6863310073061781342   \n",
       "1           TeraSky Baltic Rebilling - Customer List  2798679710274239636   \n",
       "2  VCF Migration Plan for Existing vSphere Enviro...  2454216044330707803   \n",
       "3  VCF Migration Plan for Existing vSphere Enviro... -4490931525388484148   \n",
       "4                               Vinted License Count  8789196147665239826   \n",
       "\n",
       "         operation geolocation        client_ip  \\\n",
       "0  FILE_DOWNLOADED       US-VA  172.190.156.130   \n",
       "1  FILE_DOWNLOADED       US-VA  172.190.156.130   \n",
       "2  FILE_DOWNLOADED       US-VA  172.190.156.130   \n",
       "3  FILE_DOWNLOADED       US-VA  172.190.156.130   \n",
       "4             READ         NaN              NaN   \n",
       "\n",
       "                                             user_id  user_agent batch_id  \\\n",
       "0                                  gabib@terasky.com         NaN      NaN   \n",
       "1                              gediminas@terasky.com         NaN      NaN   \n",
       "2                                mandeep@terasky.com         NaN      NaN   \n",
       "3                                mandeep@terasky.com         NaN      NaN   \n",
       "4  google-workspace-api@vinted-it.iam.gserviceacc...         NaN      NaN   \n",
       "\n",
       "   auditor_id  audited_id application_name  \n",
       "0      813555   2443907.0              NaN  \n",
       "1      813555   2444061.0              NaN  \n",
       "2      813555   2444217.0              NaN  \n",
       "3      813555   2444217.0              NaN  \n",
       "4      813555   2443585.0              NaN  "
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464c010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterate Audit Log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1109643 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "2443907.0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[341]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     79\u001b[39m       \u001b[38;5;28;01mdel\u001b[39;00m meta[\u001b[33m'\u001b[39m\u001b[33mtimes\u001b[39m\u001b[33m'\u001b[39m][OP]\n\u001b[32m     81\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m files\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m files = \u001b[43mget_meta_features_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[341]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mget_meta_features_files\u001b[39m\u001b[34m(files, num_top_locs)\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m file_info[\u001b[33m'\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIterate Audit Log\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mget_file_info_from_auditlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUpdate Files Metadata\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file, file_info \u001b[38;5;129;01min\u001b[39;00m tqdm(files.items(), total=\u001b[38;5;28mlen\u001b[39m(files)):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[341]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mget_file_info_from_auditlog\u001b[39m\u001b[34m(files, ra)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m tqdm(ra.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(ra)):\n\u001b[32m      5\u001b[39m   row_info = row[\u001b[32m1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m   file_hash = \u001b[43maudit_to_file_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow_info\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maudited_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m      9\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m file_hash \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDidn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find file hash in files... \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_hash\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 2443907.0"
     ]
    }
   ],
   "source": [
    "# FILE: Extract meta features from audit log\n",
    "\n",
    "def get_interarrivals(all_ops):\n",
    "  interarrivals = []\n",
    "  if len(all_ops) <= 1:\n",
    "    return []\n",
    "  start_time = all_ops[0]\n",
    "  for next_time in all_ops[1:]:\n",
    "    diff = (next_time - start_time).total_seconds()\n",
    "    assert diff >= 0, f\"not ordered: start_time: {start_time}, next_time: {next_time}\"\n",
    "    interarrivals.append(diff)\n",
    "    start_time = next_time\n",
    "  return interarrivals\n",
    "\n",
    "def get_sit(interarrivals, mean):\n",
    "  deviations = [math.pow(i - mean, 2) for i in interarrivals]\n",
    "  if len(deviations) == 0:\n",
    "    return -1\n",
    "  return math.sqrt(sum(deviations) / len(deviations))\n",
    "\n",
    "def get_file_info_from_auditlog(files, ra):\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "\n",
    "    file_hash = audit_to_file_mapping[row_info['audited_id']]\n",
    "\n",
    "    if file_hash not in files:\n",
    "      print(f\"Didn't find file hash in files... {file_hash}\")\n",
    "      continue\n",
    "    file_info = files[file_hash]\n",
    "    \n",
    "    # Create metadata object for user if doesn't exist\n",
    "    metadata = file_info.setdefault(\"metadata\", {})\n",
    "    \n",
    "    # Add location information\n",
    "    locs = metadata.setdefault('locs', list())\n",
    "    locs.append(f'{row_info['geolocation']}-{row_info['client_ip']}')\n",
    "\n",
    "    # Add timestamps per operations\n",
    "    operation = row_info['operation']\n",
    "    times = metadata.setdefault('times', {})\n",
    "\n",
    "    # TODO: Include user information in operation\n",
    "    all_operations = times.setdefault('all_ops', [])\n",
    "    operation_times = times.setdefault(operation, [])\n",
    "    timestamp = datetime.fromisoformat(row_info['timestamp'])\n",
    "    # Per operation\n",
    "    operation_times.append(timestamp)\n",
    "    # All operations\n",
    "    all_operations.append(timestamp)\n",
    "  \n",
    "  for file, file_info in files.items():\n",
    "    metadata = file_info['metadata']\n",
    "    # Ensure times are sorted...\n",
    "    file_info['times']['all_ops'] = sorted(file_info['times']['all_ops'])\n",
    "    all_ops = file_info['times']['all_ops']\n",
    "    interarrivals = get_interarrivals(file_info['times']['all_ops'])\n",
    "    # mean interarrival time\n",
    "    metadata['mit'] = sum(interarrivals) / len(interarrivals) if len(interarrivals) > 0 else -1\n",
    "    # std interarrival time\n",
    "    metadata['sit'] = get_sit(interarrivals, metadata['mit'])\n",
    "    # recency\n",
    "    metadata['recency'] = (NOW - all_ops[-1]).total_seconds()\n",
    "    # user distribution entropy\n",
    "    metadata['ud_entropy'] = 0\n",
    "    # unique users\n",
    "    metadata['unique_users'] = 1\n",
    "    # top users\n",
    "    metadata['top_users'] = 1\n",
    "\n",
    "def get_meta_features_files(files, num_top_locs=3):\n",
    "  ra = dfs['resource_auditrecord']\n",
    "  # ra = ra.head(10000)\n",
    "\n",
    "  UNIQUE_OPS = ra['operation'].unique()\n",
    "\n",
    "  for file, file_info in files.items():\n",
    "    if file_info.get(\"metadata\"):\n",
    "      del file_info['metadata']\n",
    "\n",
    "  print(\"Iterate Audit Log\")\n",
    "  get_file_info_from_auditlog(files, ra)\n",
    "\n",
    "  print(\"Update Files Metadata\")\n",
    "  for file, file_info in tqdm(files.items(), total=len(files)):\n",
    "    if not file_info.get('metadata'):\n",
    "      print(f\"No metadata for user {file}\")\n",
    "      continue\n",
    "    meta = file_info['metadata']\n",
    "    if not meta.get('locs'):\n",
    "      print(f\"No locs for user {file}\")\n",
    "      continue\n",
    "\n",
    "    # unique locations\n",
    "    meta['unique_locs'] = len(set(meta['locs']))\n",
    "\n",
    "    # top locations\n",
    "    counter = Counter(meta['locs'])\n",
    "    top_locs = counter.most_common(num_top_locs)\n",
    "    for i, loc in enumerate(top_locs):\n",
    "      meta[f'top_loc_{i}'] = loc[0]\n",
    "      meta[f'top_loc_{i}_per'] = loc[1] / len(meta['locs'])\n",
    "\n",
    "    # time meta\n",
    "    for OP in UNIQUE_OPS:\n",
    "      if not meta.get(\"times\"):\n",
    "        break\n",
    "      times = meta[\"times\"]\n",
    "      if not times.get(OP):\n",
    "        times[OP] = []\n",
    "      \n",
    "      # time buckets and FFT\n",
    "      get_time_based_meta(NOW - timedelta(days=7), NOW, timedelta(hours=1), meta, OP, 'hour')\n",
    "      get_time_based_meta(NOW - timedelta(weeks=4), NOW, timedelta(days=1), meta, OP, 'day')\n",
    "      get_time_based_meta(NOW - timedelta(weeks=52), NOW, timedelta(weeks=4), meta, OP, 'month')\n",
    "\n",
    "    get_burstiness(get_interarrivals(file_info['file_infos']), meta)\n",
    "\n",
    "    # Clean up metadata we don't need\n",
    "    del meta['locs']\n",
    "    for OP in UNIQUE_OPS:\n",
    "      del meta['times'][OP]\n",
    "\n",
    "  return files\n",
    "\n",
    "files = get_meta_features_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c36f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "users[list(users.keys())[1]]['metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095ef0d",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5c98d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05757612",
   "metadata": {},
   "source": [
    "# Quality Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998f412",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb5b98b",
   "metadata": {},
   "source": [
    "# Representing Users and Files\n",
    "#### Design Document: https://docs.google.com/document/d/1F84Nj3IQ-f_36bmmsOTuOo9u65gfD1WU5LKdnw8ShcY/edit?tab=t.x97j5jy1kop1#heading=h.1vu1g9fe3ujo\n",
    "#### Optimizations:\n",
    "- Re-ranking topics\n",
    "- More information in file embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5988742d",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af224c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install openai\n",
    "! pip install sentence-transformers\n",
    "! pip install torch\n",
    "! pip install matplotlib\n",
    "! pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9430db1",
   "metadata": {},
   "source": [
    "# Utilities/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV files representing database into pandas DataFrames\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "CSV_DIR = \"./csvs\"\n",
    "\n",
    "DFS = {}\n",
    "for filename in os.listdir(CSV_DIR):\n",
    "    path = CSV_DIR + \"/\" + filename\n",
    "    try:\n",
    "        name_no_ext = filename.split('.')[0]\n",
    "        DFS[name_no_ext] = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {path}: {e}\")\n",
    "\n",
    "# Print brief summary and show first few rows for each loaded dataframe\n",
    "from IPython.display import display\n",
    "\n",
    "for name, df in DFS.items():\n",
    "    print(f\"{name}: {df.shape}\")\n",
    "    display(df.head())\n",
    "\n",
    "UNIQUE_OPS = DFS['resource_auditrecord']['operation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89901229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Now for this session\n",
    "NOW = datetime.now(timezone.utc)\n",
    "\n",
    "# Cached embeddings directory\n",
    "CACHE_DIR = \"./pickle/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87fadaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache any heavy embeddings or mappings we compute.\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "def _most_recent_file(pattern):\n",
    "  # Return the most recently modified file in cache_dir matching the glob pattern, or None if none found\n",
    "  matches = glob.glob(os.path.join(CACHE_DIR, pattern))\n",
    "  if not matches:\n",
    "    return None\n",
    "  return max(matches, key=os.path.getmtime)\n",
    "\n",
    "def get_cache():\n",
    "  audit_to_file_mapping_file_path = _most_recent_file(\"audit_to_file_mapping_*.pkl\")\n",
    "  topic_embeddings_file_path = _most_recent_file(\"topic_embeddings_bge-large-zh-v1.5_*.pkl\")\n",
    "  file_embeddings_file_path = _most_recent_file(\"file_embeddings_*.pkl\")\n",
    "  user_embeddings_file_path = _most_recent_file(\"user_embeddings_*.pkl\")\n",
    "\n",
    "  audit_to_file_mapping, embeddings_cache, files, users = {}, {}, {}, {}\n",
    "\n",
    "  if audit_to_file_mapping_file_path and os.path.exists(audit_to_file_mapping_file_path):\n",
    "    with open(audit_to_file_mapping_file_path, \"rb\") as p:\n",
    "      audit_to_file_mapping = pickle.load(p)\n",
    "\n",
    "  if topic_embeddings_file_path and os.path.exists(topic_embeddings_file_path):\n",
    "    with open(topic_embeddings_file_path, \"rb\") as p:\n",
    "      embeddings_cache = pickle.load(p)\n",
    "\n",
    "  if file_embeddings_file_path and os.path.exists(file_embeddings_file_path):\n",
    "    with open(file_embeddings_file_path, \"rb\") as p:\n",
    "      files = pickle.load(p)\n",
    "\n",
    "  if user_embeddings_file_path and os.path.exists(user_embeddings_file_path):\n",
    "    with open(user_embeddings_file_path, \"rb\") as p:\n",
    "      users = pickle.load(p)\n",
    "\n",
    "  print(f\"Audit to file mapping size: {len(audit_to_file_mapping)}\")\n",
    "  print(f\"Topic embeddings cache size: {len(embeddings_cache) if embeddings_cache is not None else 0}\")\n",
    "  print(f\"Files size: {len(files) if files is not None else 0}\")\n",
    "  print(f\"Users size: {len(users) if users is not None else 0}\")\n",
    "\n",
    "  return audit_to_file_mapping, embeddings_cache, files, users\n",
    "\n",
    "\n",
    "def save_to_cache(audit_to_file_mapping, embeddings_cache, files, users):\n",
    "  # Ensure cache dir exists\n",
    "  os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "  save_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "  with open(os.path.join(CACHE_DIR, f\"topic_embeddings_bge-large-zh-v1.5_{save_time}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(embeddings_cache, f)\n",
    "  with open(os.path.join(CACHE_DIR, f\"file_embeddings_{save_time}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(files, f)\n",
    "  with open(os.path.join(CACHE_DIR, f\"user_embeddings_{save_time}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(users, f)\n",
    "  with open(os.path.join(CACHE_DIR, f\"audit_to_file_mapping_{save_time}.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(audit_to_file_mapping, f)\n",
    "\n",
    "AUDIT_TO_FILE_MAPPING, EMBEDDINGS_CACHE, FILES, USERS = get_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c842ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database searching utilities\n",
    "from functools import cache\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Getting audit ids which are associated with a file we have information for\n",
    "# This way we have a much smaller list of audit logs to parse when we are\n",
    "# calculating user embeddings\n",
    "@cache\n",
    "def get_valid_audit_ids():\n",
    "  rr, rrn, ra = DFS[\"resource_resource\"], DFS[\"resource_resourcenode\"], DFS[\"resource_auditrecord\"]\n",
    "  valid_resource_ids = set()\n",
    "  print(\"Get valid resources\")\n",
    "  for row in tqdm(rr.iterrows(), total=len(rr)):\n",
    "    row_info = row[1]\n",
    "    id = row_info['id']\n",
    "\n",
    "    if len(rr[rr['parent_id'] == id]) != 0:\n",
    "      valid_resource_ids.add(id)\n",
    "\n",
    "  valid_resource_node_ids = set()\n",
    "  print(\"Get valid resource nodes\")\n",
    "  for row in tqdm(rrn.iterrows(), total=len(rrn)):\n",
    "    row_info = row[1]\n",
    "    if row_info['resource_id'] in valid_resource_ids:\n",
    "      valid_resource_node_ids.add(row_info['id'])\n",
    "\n",
    "  valid_audit_ids = set()\n",
    "  ra.head()\n",
    "  print(\"Get valid audit ids\")\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "    if row_info['audited_id'] in valid_resource_node_ids:\n",
    "      valid_audit_ids.add(row_info['id'])\n",
    "  return valid_audit_ids, valid_resource_node_ids, valid_resource_ids\n",
    "\n",
    "def get_stream(audit_id, file_id, streams, timestamp, operation):\n",
    "  # File has been changed in some way, we want to find the first STREAM that is after the audit log timestamp\n",
    "  should_check_after = operation in ['MODIFIED', 'FILE_UPLOADED', 'RENAMED']\n",
    "  \n",
    "  # 1 stream, always pick it\n",
    "  if len(streams) == 1:\n",
    "    stream = streams.iloc[0]\n",
    "  else:\n",
    "    streams = streams.sort_values(by=\"timestamp\")\n",
    "    if should_check_after:\n",
    "      filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "      if len(filtered_streams) == 0:\n",
    "        # Fallback to filtering opposite way\n",
    "        filtered_streams = streams[streams['timestamp'] <= timestamp]\n",
    "        stream = filtered_streams.iloc[-1]\n",
    "      else:\n",
    "        stream = filtered_streams.iloc[0]\n",
    "    else:\n",
    "      filtered_streams = streams[streams['timestamp'] <= timestamp]\n",
    "      if len(filtered_streams) == 0:\n",
    "        # Fallback to filtering opposite way\n",
    "        filtered_streams = streams[streams['timestamp'] >= timestamp]\n",
    "        stream = filtered_streams.iloc[0]\n",
    "      else:\n",
    "        stream = filtered_streams.iloc[-1]\n",
    "  return stream\n",
    "\n",
    "def get_file_hash_as_of_audit(audit_id, operation, timestamp):\n",
    "  rrn, rr = DFS['resource_resourcenode'], DFS['resource_resource']\n",
    "  resource_node = rrn[rrn['id'] == audit_id]\n",
    "  resource_id = resource_node['resource_id']\n",
    "\n",
    "  # Assume only one resource id associated with audit id?\n",
    "  if len(resource_id) == 0:\n",
    "    raise ValueError(f\"No resource id found. Audit id: {audit_id}\")\n",
    "  resource_id = resource_id.iloc[0]\n",
    "\n",
    "  # Grab the resource reference by the audit log event\n",
    "  resource = rr[rr['id'] == resource_id]\n",
    "  if len(resource) == 0:\n",
    "    raise ValueError(f\"No resource found: Audit id: {audit_id}\")\n",
    "\n",
    "  # Work our way down to the relevant STREAM, or in other words actual data, relevant for this audit log event\n",
    "  file_or_stream = resource.iloc[0]\n",
    "\n",
    "  if file_or_stream['resource_type'] != \"STREAM\":\n",
    "      # Must be a file\n",
    "      assert file_or_stream['resource_type'] == \"FILE\"\n",
    "      file = file_or_stream\n",
    "      streams = rr[rr['parent_id'] == file['id']]\n",
    "      if len(streams) is None:\n",
    "        # If no streams, at least return file id\n",
    "        return file['id'], False\n",
    "      stream = get_stream(audit_id, file['id'], streams, timestamp, operation)\n",
    "      hash_id = stream['hash_id']\n",
    "  else:\n",
    "      # If stream, directly get hash id\n",
    "      hash_id = file_or_stream['hash_id']\n",
    "\n",
    "  return hash_id, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb592e66",
   "metadata": {},
   "source": [
    "# User/File Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model_name = 'BAAI/bge-large-zh-v1.5'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE/TOPIC embeddings (get topic embeddings as we calculate file embeddings)\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def average_embeddings(embeddings):\n",
    "  return np.mean(embeddings, axis=0)\n",
    "\n",
    "def get_embedding_for_text(text: str):\n",
    "  text = text.lower()\n",
    "\n",
    "  if text in EMBEDDINGS_CACHE:\n",
    "    return EMBEDDINGS_CACHE[text]\n",
    "  \n",
    "  EMBEDDINGS_CACHE[text] = model.encode(text, normalize_embeddings=True) # normalize for cosine similarity\n",
    "  return EMBEDDINGS_CACHE[text]\n",
    "\n",
    "def get_file_embeddings():\n",
    "  files = {}\n",
    "  rl = DFS['resource_label']\n",
    "  rl = rl[rl['name'] == \"topic\"]\n",
    "\n",
    "  print(\"Working through resource labels\")\n",
    "  for row in tqdm(rl.iterrows(), total=len(rl)):\n",
    "\n",
    "    resource_info = row[1]\n",
    "    file_info = files.setdefault(resource_info['hash_id'], {\n",
    "      \"labels\": [],\n",
    "      \"embedding\": []\n",
    "    })\n",
    "\n",
    "    if (resource_info[\"name\"] != \"topic\"):\n",
    "      continue\n",
    "\n",
    "    file_info[\"labels\"].append({\n",
    "      \"id\": resource_info[\"id\"],\n",
    "      \"name\": resource_info[\"value\"]\n",
    "    })\n",
    "\n",
    "  print(\"Average topic embeddings for files\")\n",
    "  for _, info in tqdm(files.items(), total=len(files)):\n",
    "    info['embedding'] = average_embeddings([get_embedding_for_text(label['name']) for label in info['labels']])\n",
    "  return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411d888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER embeddings, for each time-weight the relevant file-embeddings. Create dict of {user_id: embedding}\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def aggregate_file_embeddings_per_user(valid_audit_ids):\n",
    "  users = {}\n",
    "  ar = DFS['resource_auditrecord']\n",
    "\n",
    "  ar = ar[ar['id'].isin(valid_audit_ids)]\n",
    "  \n",
    "  bad_resource_ids = set()\n",
    "  good_resource_ids = set()\n",
    "\n",
    "  print(\"Processing audit log records:\")\n",
    "  for row in tqdm(ar.iterrows(), total=len(ar)):\n",
    "    \n",
    "    row_info = row[1]\n",
    "    if row_info['audited_id'] in bad_resource_ids:\n",
    "      continue\n",
    "    \n",
    "    user_info = users.setdefault(row_info['user_id'], {\n",
    "      \"file_infos\": []\n",
    "    })\n",
    "    time_of_operation = row_info[\"timestamp\"]\n",
    "    try:\n",
    "      # Get most relevant file version as of audit time\n",
    "      hash_id, found_streams = get_file_hash_as_of_audit(row_info['audited_id'], row_info['operation'], time_of_operation)\n",
    "    except Exception as e:\n",
    "      # print(f\"Exception caught: {e}\")\n",
    "      bad_resource_ids.add(row_info['audited_id'])\n",
    "      continue\n",
    "\n",
    "    # Cache audit id to file mapping for later metadata processing, even when no streams found\n",
    "    AUDIT_TO_FILE_MAPPING[row_info['id']] = hash_id\n",
    "\n",
    "    if hash_id not in FILES:\n",
    "      # print(f\"Didn't find {hash_id} in file, audit id: {row_info['audited_id']}, must have failed topic extraction\")\n",
    "      continue\n",
    "\n",
    "    good_resource_ids.add(row_info['audited_id'])\n",
    "\n",
    "    # If successfully got hash, lookup in files table and add to user info\n",
    "    file_info = FILES[hash_id]\n",
    "    file_embedding = file_info[\"embedding\"]\n",
    "    user_info['file_infos'].append({\n",
    "      \"timestamp\": datetime.fromisoformat(time_of_operation),\n",
    "      \"embedding\": file_embedding\n",
    "    })\n",
    "\n",
    "  # print(f\"Success: {len(good_resource_ids)}, Error: {len(bad_resource_ids)}\")\n",
    "  return users\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def get_user_embedding(user_info, tau=60*60*24*30): \n",
    "  '''\n",
    "  1) Default tau to 1 month, in other words, we lose 37% of old information if \n",
    "     a month has passed since last indexing. Exponential decay.\n",
    "  2) Unit of time is seconds\n",
    "  '''\n",
    "\n",
    "  user_embedding = np.array(user_info['file_infos'][0]['embedding'])\n",
    "  embedding_time = user_info['file_infos'][0]['timestamp']\n",
    "\n",
    "  # Build up the index recursively\n",
    "  for file_info in user_info['file_infos'][1:]:\n",
    "    # Get difference from last time and update time variable\n",
    "    d_t = file_info['timestamp'] - embedding_time\n",
    "    d_t = d_t.total_seconds()\n",
    "    embedding_time = file_info['timestamp']\n",
    "\n",
    "    assert d_t >= 0, f\"User file access times must be ordered: from - {embedding_time}, to - {file_info['timestamp']}\"\n",
    "\n",
    "    # Apply the weighting\n",
    "    # d_t and tau should both be in seconds\n",
    "    alpha = 1 - math.exp(-d_t / tau)\n",
    "    user_embedding = alpha * np.array(file_info['embedding']) + (1 - alpha) * user_embedding\n",
    "\n",
    "  return user_embedding\n",
    "\n",
    "def get_user_embeddings(users):\n",
    "  # 2.1) Time weight the file embeddings\n",
    "  print(\"Calculating user embeddings: \")\n",
    "  for user, user_info in tqdm(users.items()):\n",
    "    user_info['file_infos'] = sorted(user_info['file_infos'], key=lambda event: event['timestamp'])\n",
    "    if len(user_info['file_infos']) == 0: \n",
    "      print(f\"User has no files {user}\")\n",
    "      continue\n",
    "    user_info[\"embedding\"] = get_user_embedding(user_info)\n",
    "  return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb5467",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(AUDIT_TO_FILE_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e0d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS[list(USERS.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500456c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES, USERS, AUDIT_TO_FILE_MAPPING, EMBEDDINGS_CACHE = None, None, dict(), EMBEDDINGS_CACHE\n",
    "FILES = get_file_embeddings()\n",
    "\n",
    "# This step is an optimization, don't bother processing audit ids associated with files that haven't been indexed\n",
    "valid_audit_ids, _, _ = get_valid_audit_ids()\n",
    "USERS_AGG = aggregate_file_embeddings_per_user(valid_audit_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1003acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "USERS = get_user_embeddings(USERS_AGG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_cache(AUDIT_TO_FILE_MAPPING, EMBEDDINGS_CACHE, FILES, USERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fc145",
   "metadata": {},
   "source": [
    "# Meta Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baacc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta helpers\n",
    "import math\n",
    "from collections import Counter\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def get_interarrivals(timestamps):\n",
    "  interarrivals = []\n",
    "  if len(timestamps) <= 1:\n",
    "    return []\n",
    "  prev_timestamp = timestamps[0]\n",
    "  for next_timestamp in timestamps[1:]:\n",
    "    diff = (next_timestamp - prev_timestamp).total_seconds()\n",
    "    assert diff >= 0, f\"not ordered: start_time: {prev_timestamp}, next_time: {next_timestamp}\"\n",
    "    interarrivals.append(diff)\n",
    "    prev_timestamp = next_timestamp\n",
    "  return interarrivals\n",
    "\n",
    "def get_mit(interarrivals):\n",
    "  return sum(interarrivals) / len(interarrivals) if len(interarrivals) > 0 else -1\n",
    "\n",
    "def get_sit(interarrivals, mean):\n",
    "  deviations = [math.pow(i - mean, 2) for i in interarrivals]\n",
    "  if len(deviations) == 0:\n",
    "    return -1\n",
    "  return math.sqrt(sum(deviations) / len(deviations))\n",
    "\n",
    "def get_fft_info(buckets):\n",
    "  # Calculate top 3 frequencies/amplitudes/phases of time series using FFT\n",
    "  bucket_arr = np.array(buckets)\n",
    "  fft_result = np.fft.fft(bucket_arr)\n",
    "  fft_freqs = np.fft.fftfreq(len(bucket_arr), d=1)\n",
    "  amplitudes = np.abs(fft_result)\n",
    "  phases = np.angle(fft_result)\n",
    "  # Ignore the zero frequency (DC component)\n",
    "  indices = np.argsort(amplitudes[1:])[::-1][:3] + 1 if len(amplitudes) > 1 else []\n",
    "  return [(fft_freqs[i], amplitudes[i], phases[i]) for rank, i in enumerate(indices)]\n",
    "\n",
    "def get_burstiness(interarrivals, meta):\n",
    "  # TODO\n",
    "  pass\n",
    "\n",
    "def get_time_based_meta(start, end, interval, meta, operation, time_str, times):\n",
    "  # Sort datetimes so it's O(n) operation to create buckets\n",
    "  buckets = []\n",
    "  current = start\n",
    "  idx = 0\n",
    "  n = len(times)\n",
    "  while current < end:\n",
    "      next_bucket = current + interval\n",
    "      count = 0\n",
    "      # Count how many times fall into [current, next_bucket)\n",
    "      while idx < n and times[idx] < next_bucket:\n",
    "          if times[idx] >= current:\n",
    "              count += 1\n",
    "          idx += 1\n",
    "      buckets.append(count)\n",
    "      current = next_bucket\n",
    "  \n",
    "  for i, bucket in enumerate(buckets):\n",
    "    meta['times'][f'{operation}_{time_str}_{i}'] = bucket\n",
    "  fft_infos = get_fft_info(buckets)\n",
    "  for i, info in enumerate(fft_infos):\n",
    "    meta['times'][f'{operation}_{time_str}_freq_{i}'] = info[0]\n",
    "    meta['times'][f'{operation}_{time_str}_amp_{i}'] = info[1]\n",
    "    meta['times'][f'{operation}_{time_str}_phase_{i}'] = info[2]\n",
    "\n",
    "def get_entropy(values, normalized=True):\n",
    "  if not values:\n",
    "    return 0.0\n",
    "  counts = Counter(values)\n",
    "  total = sum(counts.values())\n",
    "  probs = [cnt / total for cnt in counts.values()]\n",
    "  H = -sum(p * math.log2(p) for p in probs if p > 0)\n",
    "\n",
    "  if not normalized:\n",
    "      return H\n",
    "\n",
    "  k = len(counts)\n",
    "  if k <= 1:\n",
    "      return 0.0\n",
    "  return H / math.log2(k)\n",
    "\n",
    "def setup_metadata_from_audit_log(metadata, row_info):\n",
    "  # Add location information\n",
    "  locs = metadata.setdefault('locs', list())\n",
    "  locs.append(f'{row_info['geolocation']}-{row_info['client_ip']}')\n",
    "\n",
    "  # Add timestamps for operations\n",
    "  operation = row_info['operation']\n",
    "  times = metadata.setdefault('times', {})\n",
    "\n",
    "  all_operations = times.setdefault('all_ops', [])\n",
    "  operation_times = times.setdefault(operation, [])\n",
    "  event = {\n",
    "     \"timestamp\": datetime.fromisoformat(row_info['timestamp']),\n",
    "     \"user\": row_info[\"user_id\"]\n",
    "  }\n",
    "  # Per operation\n",
    "  operation_times.append(event)\n",
    "  # All operations\n",
    "  all_operations.append(event)\n",
    "\n",
    "def fill_metadata_stats_for_file_or_users(metadata, users, num_top=3):\n",
    "  sorted_ops = [o['timestamp'] for o in sorted(metadata['times']['all_ops'], key=lambda op: op['timestamp'])]\n",
    "  interarrivals = get_interarrivals(sorted_ops)\n",
    "  # mean interarrival time\n",
    "  metadata['mit'] = sum(interarrivals) / len(interarrivals) if len(interarrivals) > 0 else -1\n",
    "  # std interarrival time\n",
    "  metadata['sit'] = get_sit(interarrivals, metadata['mit'])\n",
    "  # recency\n",
    "  metadata['recency'] = (NOW - sorted_ops[-1]).total_seconds()\n",
    "\n",
    "  # user distribution entropy\n",
    "  metadata['ud_entropy'] = get_entropy(users)\n",
    "  # unique users\n",
    "  metadata['unique_users'] = len(set(users))\n",
    "  # top users\n",
    "  counter = Counter(users)\n",
    "  top_users = counter.most_common(num_top)\n",
    "  for i, user in enumerate(top_users):\n",
    "    metadata[f'top_user_{i}'] = user[0]\n",
    "    metadata[f'top_user_{i}_per'] = user[1] / len(users)\n",
    "\n",
    "  # unique locations\n",
    "  metadata['unique_locs'] = len(set(metadata['locs']))\n",
    "\n",
    "  # top locations\n",
    "  counter = Counter(metadata['locs'])\n",
    "  top_locs = counter.most_common(num_top)\n",
    "  for i, loc in enumerate(top_locs):\n",
    "    metadata[f'top_loc_{i}'] = loc[0]\n",
    "    metadata[f'top_loc_{i}_per'] = loc[1] / len(metadata['locs'])\n",
    "\n",
    "  # time meta\n",
    "  for OP in UNIQUE_OPS:\n",
    "    if not metadata.get(\"times\"):\n",
    "      break\n",
    "    op_times = []\n",
    "    if metadata[\"times\"].get(OP):\n",
    "      op_times = [t['timestamp'] for t in sorted([op for op in metadata[\"times\"][OP]], key=lambda operation: operation[\"timestamp\"])]\n",
    "\n",
    "    \n",
    "    # time buckets and FFT for specific operations\n",
    "    get_time_based_meta(NOW - timedelta(days=7), NOW, timedelta(hours=1), metadata, OP, 'hour', op_times)\n",
    "    get_time_based_meta(NOW - timedelta(weeks=4), NOW, timedelta(days=1), metadata, OP, 'day', op_times)\n",
    "    get_time_based_meta(NOW - timedelta(weeks=52), NOW, timedelta(weeks=4), metadata, OP, 'month', op_times)\n",
    "\n",
    "  # time buckets and FFT for 'all' operations\n",
    "  get_time_based_meta(NOW - timedelta(days=7), NOW, timedelta(hours=1), metadata, \"all_ops\", 'hour', sorted_ops)\n",
    "  get_time_based_meta(NOW - timedelta(weeks=4), NOW, timedelta(days=1), metadata, \"all_ops\", 'day', sorted_ops)\n",
    "  get_time_based_meta(NOW - timedelta(weeks=52), NOW, timedelta(weeks=4), metadata, \"all_ops\", 'month', sorted_ops)\n",
    "\n",
    "  # burstiness stats\n",
    "  get_burstiness(interarrivals, metadata)\n",
    "\n",
    "def clean_up_metadata(metadata):\n",
    "  # Clean up metadata we don't need\n",
    "  del metadata['locs']\n",
    "  del metadata['times']['all_ops']\n",
    "  for OP in UNIQUE_OPS:\n",
    "    if metadata['times'].get(OP):\n",
    "      del metadata['times'][OP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b38fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER: Extract meta features from audit log\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_user_info_from_auditlog(users, ra):\n",
    "  print(\"Iterate Audit Log\")\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "    uid = row_info['user_id']\n",
    "    user_info = {\n",
    "      \"file_infos\": [],\n",
    "      \"embedding\": []\n",
    "    }\n",
    "    if uid in users:\n",
    "      user_info: dict = users[uid]\n",
    "    \n",
    "    # Create metadata object for user if doesn't exist\n",
    "    metadata = user_info.setdefault(\"metadata\", {})\n",
    "    \n",
    "    setup_metadata_from_audit_log(metadata, row_info)\n",
    "\n",
    "\n",
    "def get_meta_features_users(users, num_top_locs=3, limit=1000000000):\n",
    "  ra = DFS['resource_auditrecord']\n",
    "  ra = ra.head(limit)\n",
    "\n",
    "  # Delete any existing metadata so we don't mix between runs\n",
    "  for user, user_info in users.items():\n",
    "    if user_info.get(\"metadata\"):\n",
    "      del user_info['metadata']\n",
    "\n",
    "  get_user_info_from_auditlog(users, ra)\n",
    "\n",
    "  print(\"Update Users Metadata\")\n",
    "  for user, user_info in tqdm(users.items(), total=len(users)):\n",
    "    if not user_info.get('metadata'):\n",
    "      # print(f\"No metadata for file {file}\")\n",
    "      continue\n",
    "    metadata = user_info['metadata']\n",
    "    if not metadata.get('locs'):\n",
    "      print(f\"No activity for user {user}\")\n",
    "      continue\n",
    "\n",
    "    fill_metadata_stats_for_file_or_users(metadata, [user])\n",
    "    clean_up_metadata(metadata)\n",
    "\n",
    "  return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6464c010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILE: Extract meta features from audit log\n",
    "from collections import Counter\n",
    "\n",
    "def get_file_info_from_auditlog(files, ra):\n",
    "  print(\"Iterate Audit Log\")\n",
    "  for row in tqdm(ra.iterrows(), total=len(ra)):\n",
    "    row_info = row[1]\n",
    "\n",
    "    audit_log_id = row_info['id']\n",
    "    if audit_log_id not in AUDIT_TO_FILE_MAPPING:\n",
    "      # print(\"No file info available\")\n",
    "      continue\n",
    "    file_hash = AUDIT_TO_FILE_MAPPING[row_info['id']]\n",
    "\n",
    "    if file_hash not in files:\n",
    "      # print(f\"Didn't find file hash in files... adding file\")\n",
    "      files[file_hash] = {}\n",
    "    file_info = files[file_hash]\n",
    "    \n",
    "    # Create metadata object for file if doesn't exist\n",
    "    metadata = file_info.setdefault(\"metadata\", {})\n",
    "\n",
    "    setup_metadata_from_audit_log(metadata, row_info)\n",
    "\n",
    "def get_meta_features_files(files, num_top_locs=3, limit=1000000000):\n",
    "  ra = DFS['resource_auditrecord']\n",
    "  ra = ra.head(limit)\n",
    "\n",
    "  # Delete any existing metadata so we don't mix between runs\n",
    "  for file, file_info in files.items():\n",
    "    if file_info.get(\"metadata\"):\n",
    "      del file_info['metadata']\n",
    "\n",
    "  get_file_info_from_auditlog(files, ra)\n",
    "\n",
    "  print(\"Update Files Metadata\")\n",
    "  for file, file_info in tqdm(files.items(), total=len(files)):\n",
    "    if not file_info.get('metadata'):\n",
    "      # print(f\"No metadata for file {file}\")\n",
    "      continue\n",
    "    metadata = file_info['metadata']\n",
    "    if not metadata.get('locs'):\n",
    "      # print(f\"No activity for file {file}\")\n",
    "      continue\n",
    "\n",
    "    users = [op['user'] for op in metadata['times']['all_ops']]\n",
    "    fill_metadata_stats_for_file_or_users(metadata, users)\n",
    "    clean_up_metadata(metadata)\n",
    "  return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b988206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 10000000000000\n",
    "USERS = get_meta_features_users(USERS, limit=limit)\n",
    "FILES = get_meta_features_files(FILES, limit=limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02d5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_cache(AUDIT_TO_FILE_MAPPING, EMBEDDINGS_CACHE, FILES, USERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c36f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "ra = DFS['resource_auditrecord']\n",
    "rrn = DFS['resource_resourcenode']\n",
    "rr = DFS['resource_resource']\n",
    "rl = DFS['resource_label']\n",
    "rh = DFS['resource_hash']\n",
    "# rrn[rrn['id'] == 2444061]\n",
    "# rr[rr['id'] == 814033]\n",
    "# rr[rr['parent_id'] == 814033]\n",
    "# rl[rl['hash_id'] == 78048]\n",
    "# ra.head()\n",
    "# ra[ra['audited_id'] == 2444061.0]\n",
    "# len(rr['id'].unique())\n",
    "# rr[rr['resource_type'] == \"FILE\"]\n",
    "# rrn = dfs['resource_resourcenode']\n",
    "# len(rrn['resource_id'].unique())\n",
    "\n",
    "# Get valid audit record ids preemptively\n",
    "340 in set(AUDIT_TO_FILE_MAPPING.values())\n",
    "\n",
    "index = 4000\n",
    "# users_list = list(USERS.keys())\n",
    "# user_name = users_list[index]\n",
    "files_list = list(FILES.keys())\n",
    "file_hash = files_list[index]\n",
    "# USERS[user_name]['metadata']\n",
    "count = 0\n",
    "for hash, file in FILES.items():\n",
    "  if file.get(\"metadata\"):\n",
    "    count += 1\n",
    "\n",
    "print(count)\n",
    "# file_hash\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d5c98d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35776cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def perform_pca_on_metadata(data, metadata_features, n_components=None, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Perform PCA on metadata features of the dataset, properly handling both numerical and categorical features.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): The input DataFrame containing all features\n",
    "        metadata_features (list): List of column names for metadata features\n",
    "        n_components (int, optional): Number of components to keep. If None, determined by variance_threshold\n",
    "        variance_threshold (float, optional): Minimum cumulative explained variance ratio to maintain\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (transformed_data, pca_object, preprocessor_object)\n",
    "            - transformed_data: PCA-transformed features\n",
    "            - pca_object: Fitted PCA object for future use\n",
    "            - preprocessor_object: Fitted ColumnTransformer object for future use\n",
    "    \"\"\"\n",
    "    # Extract metadata features\n",
    "    X = data[metadata_features].copy()\n",
    "    \n",
    "    # Print information about missing values\n",
    "    n_missing = X.isnull().sum()\n",
    "    if n_missing.any():\n",
    "        print(\"\\nMissing values per feature:\")\n",
    "        print(n_missing[n_missing > 0])\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category', 'bool']).columns\n",
    "    \n",
    "    # Create preprocessing steps for both numeric and categorical features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(drop='if_binary', sparse_output=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ],\n",
    "        remainder='drop'  # Drop any columns that don't fit either category\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "    \n",
    "    # Get feature names after preprocessing\n",
    "    numeric_features_out = list(numeric_features)\n",
    "    if len(categorical_features) > 0:\n",
    "        categorical_features_out = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_features)\n",
    "    else:\n",
    "        categorical_features_out = []\n",
    "    feature_names = numeric_features_out + list(categorical_features_out)\n",
    "    \n",
    "    # If n_components not specified, use variance threshold\n",
    "    if n_components is None:\n",
    "        # Start with min(n_samples, n_features) components\n",
    "        n_features = min(X_preprocessed.shape[0], X_preprocessed.shape[1])\n",
    "        pca_full = PCA(n_components=n_features)\n",
    "        pca_full.fit(X_preprocessed)\n",
    "        \n",
    "        # Find number of components needed to explain variance_threshold of variance\n",
    "        cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum >= variance_threshold) + 1\n",
    "    \n",
    "    # Perform PCA with determined number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_transformed = pca.fit_transform(X_preprocessed)\n",
    "    \n",
    "    # Create DataFrame with transformed features\n",
    "    columns = [f'PC{i+1}' for i in range(n_components)]\n",
    "    X_pca = pd.DataFrame(X_transformed, columns=columns, index=data.index)\n",
    "    \n",
    "    # Print information about the transformation\n",
    "    print(f\"\\nNumber of numerical features: {len(numeric_features)}\")\n",
    "    print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Total features after one-hot encoding: {X_preprocessed.shape[1]}\")\n",
    "    print(f\"Number of PCA components: {n_components}\")\n",
    "    print(f\"Total variance explained: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    \n",
    "    # Optional: Print top feature contributions to first few components\n",
    "    if len(feature_names) > 0:\n",
    "        n_top = 3  # Number of top features to show\n",
    "        n_components_to_show = min(3, n_components)  # Number of components to show\n",
    "        \n",
    "        print(\"\\nTop feature contributions to first few principal components:\")\n",
    "        for i in range(n_components_to_show):\n",
    "            loadings = pd.Series(\n",
    "                pca.components_[i],\n",
    "                index=feature_names\n",
    "            ).abs().sort_values(ascending=False)\n",
    "            \n",
    "            print(f\"\\nPC{i+1} top {n_top} features:\")\n",
    "            print(loadings.head(n_top))\n",
    "    \n",
    "    return X_pca, pca, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5a010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate embeddings\n",
    "\n",
    "def get_df_entry(metadata_info, id, is_user):\n",
    "    entry = {}\n",
    "    entry['is_user'] = is_user\n",
    "    entry['object_id'] = id\n",
    "    for k, v in metadata_info.items():\n",
    "        if k == \"times\":\n",
    "            for time_key, time_value in v.items():\n",
    "                entry[time_key] = time_value\n",
    "        else:\n",
    "            entry[k] = v\n",
    "    return entry\n",
    "\n",
    "def create_metadata_df(users, files):\n",
    "    # Remove nan keys...\n",
    "    files = {k: v for k,v in files.items() if not np.isnan(k)}\n",
    "    # Get all metadata objects\n",
    "    metadata_objs = []\n",
    "    for user_id, user_info in users.items():\n",
    "        metadata_objs.append(get_df_entry(user_info['metadata'], user_id, True))\n",
    "    for file_hash, file_info in files.items():\n",
    "        if file_info.get(\"metadata\"):\n",
    "            metadata_objs.append(get_df_entry(file_info['metadata'], file_hash, False))\n",
    "    print(f\"Number of metadata objs {len(metadata_objs)}\")\n",
    "    df = pd.DataFrame(metadata_objs)\n",
    "    df = df.add_prefix(\"META_\")\n",
    "    return df\n",
    "\n",
    "def create_topic_embeddings_df(users, files):\n",
    "  # Add object ID and user ID so we can join with metadata embeddings\n",
    "  user_embeddings = [get_embedding(user_info) + [user_id, True] for user_id, user_info in users.items()]\n",
    "  file_embeddings = [get_embedding(file_info) + [file_id, False] for file_id, file_info in files.items()]\n",
    "  print(f\"Number of topic embeddings {len(user_embeddings) + len(file_embeddings)}\")\n",
    "  df = pd.DataFrame(user_embeddings + file_embeddings)\n",
    "  df = df.rename(columns={1024: 'object_id', 1025: 'is_user'}).add_prefix(\"TOPIC_\")\n",
    "  return df\n",
    "\n",
    "# Feature concatenation\n",
    "def get_embedding(user_info, embedding_size=1024):\n",
    "  if 'embedding' in user_info:\n",
    "    return user_info['embedding'].tolist()\n",
    "  else:\n",
    "    return [0] * embedding_size\n",
    "\n",
    "def concatenate_topic_meta_embeddings(topic_df, meta_df):\n",
    "  # Ensure object_id is the right type in both dataframes\n",
    "  topic_df['TOPIC_object_id'] = topic_df['TOPIC_object_id'].astype(str)\n",
    "  meta_df['META_object_id'] = meta_df['META_object_id'].astype(str)\n",
    "  \n",
    "  # LEFT join to keep all rows from meta_df and only matching rows from topic_df\n",
    "  merged_df = pd.merge(meta_df, topic_df, left_on=['META_object_id'], right_on=['TOPIC_object_id'], how='left')\n",
    "  return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = create_metadata_df(USERS, FILES)\n",
    "topic_embeddings_df = create_topic_embeddings_df(USERS, FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ccb9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the identification columns\n",
    "id_columns = [\"META_is_user\", \"META_object_id\"]\n",
    "features_names = list(metadata_df.columns)\n",
    "for col in id_columns:\n",
    "    features_names.remove(col)\n",
    "\n",
    "# Perform PCA\n",
    "metadata_embeddings_df, pca, preprocessor = perform_pca_on_metadata(metadata_df, features_names)\n",
    "\n",
    "# Add back the identification columns\n",
    "for col in id_columns:\n",
    "    metadata_embeddings_df[col] = metadata_df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge topic/meta embeddings\n",
    "merged_df = concatenate_topic_meta_embeddings(topic_embeddings_df, metadata_embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b743cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive loss learning\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "# Neural network to learn optimal lambda for combining embeddings\n",
    "class EmbeddingBalancer(nn.Module):\n",
    "    def __init__(self, topic_dim, meta_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.topic_encoder = nn.Sequential(\n",
    "            nn.Linear(topic_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.meta_encoder = nn.Sequential(\n",
    "            nn.Linear(meta_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        self.lambda_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()  # Lambda should be between 0 and 1\n",
    "        )\n",
    "        \n",
    "    def forward(self, topic_emb, meta_emb):\n",
    "        topic_encoded = self.topic_encoder(topic_emb)\n",
    "        meta_encoded = self.meta_encoder(meta_emb)\n",
    "        combined = torch.cat([topic_encoded, meta_encoded], dim=1)\n",
    "        lambda_weight = self.lambda_net(combined)\n",
    "        \n",
    "        # Weighted combination of embeddings\n",
    "        weighted_emb = lambda_weight * topic_emb + (1 - lambda_weight) * meta_emb\n",
    "        return weighted_emb, lambda_weight\n",
    "\n",
    "# Dataset for contrastive learning\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, merged_df, num_negatives=5):\n",
    "        self.df = merged_df\n",
    "        self.num_negatives = num_negatives\n",
    "        \n",
    "        # Separate topic and metadata embeddings\n",
    "        self.topic_cols = [str(i) for i in range(1024)]  # First 1024 columns are topic embeddings\n",
    "        self.meta_cols = [col for col in merged_df.columns if col.startswith('PC')]  # PCA components\n",
    "        \n",
    "        self.topic_embeddings = torch.FloatTensor(merged_df[self.topic_cols].values)\n",
    "        self.meta_embeddings = torch.FloatTensor(merged_df[self.meta_cols].values)\n",
    "        \n",
    "        # Group by user vs file\n",
    "        self.user_indices = merged_df[merged_df['is_user'] == True].index.tolist()\n",
    "        self.file_indices = merged_df[merged_df['is_user'] == False].index.tolist()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get anchor embeddings\n",
    "        anchor_topic = self.topic_embeddings[idx]\n",
    "        anchor_meta = self.meta_embeddings[idx]\n",
    "        is_user = idx in self.user_indices\n",
    "        \n",
    "        # Get positive example (same type - user or file)\n",
    "        pos_pool = self.user_indices if is_user else self.file_indices\n",
    "        pos_idx = np.random.choice([i for i in pos_pool if i != idx])\n",
    "        positive_topic = self.topic_embeddings[pos_idx]\n",
    "        positive_meta = self.meta_embeddings[pos_idx]\n",
    "        \n",
    "        # Get negative examples (opposite type)\n",
    "        neg_pool = self.file_indices if is_user else self.user_indices\n",
    "        neg_indices = np.random.choice(neg_pool, size=self.num_negatives)\n",
    "        negative_topics = self.topic_embeddings[neg_indices]\n",
    "        negative_metas = self.meta_embeddings[neg_indices]\n",
    "        \n",
    "        return {\n",
    "            'anchor_topic': anchor_topic,\n",
    "            'anchor_meta': anchor_meta,\n",
    "            'positive_topic': positive_topic,\n",
    "            'positive_meta': positive_meta,\n",
    "            'negative_topics': negative_topics,\n",
    "            'negative_metas': negative_metas\n",
    "        }\n",
    "\n",
    "# Contrastive loss function\n",
    "def contrastive_loss(anchor, positive, negatives, temperature=0.07):\n",
    "    # Compute similarities\n",
    "    pos_sim = torch.cosine_similarity(anchor, positive, dim=1)\n",
    "    neg_sims = torch.cosine_similarity(anchor.unsqueeze(1), negatives, dim=2)\n",
    "    \n",
    "    # Scale by temperature\n",
    "    pos_sim = pos_sim / temperature\n",
    "    neg_sims = neg_sims / temperature\n",
    "    \n",
    "    # Compute loss\n",
    "    logits = torch.cat([pos_sim.unsqueeze(1), neg_sims], dim=1)\n",
    "    labels = torch.zeros(len(anchor), dtype=torch.long, device=anchor.device)  # First index (0) is positive\n",
    "    return nn.CrossEntropyLoss()(logits, labels)\n",
    "\n",
    "# Training function\n",
    "def train_balancer(model, train_loader, optimizer, device, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            anchor_topic = batch['anchor_topic'].to(device)\n",
    "            anchor_meta = batch['anchor_meta'].to(device)\n",
    "            positive_topic = batch['positive_topic'].to(device)\n",
    "            positive_meta = batch['positive_meta'].to(device)\n",
    "            negative_topics = batch['negative_topics'].to(device)\n",
    "            negative_metas = batch['negative_metas'].to(device)\n",
    "            \n",
    "            # Forward pass for anchor and positive pairs\n",
    "            anchor_combined, anchor_lambda = model(anchor_topic, anchor_meta)\n",
    "            positive_combined, pos_lambda = model(positive_topic, positive_meta)\n",
    "            \n",
    "            # Forward pass for negatives\n",
    "            neg_combined = []\n",
    "            for i in range(negative_topics.size(1)):\n",
    "                neg_emb, neg_lambda = model(negative_topics[:, i], negative_metas[:, i])\n",
    "                neg_combined.append(neg_emb)\n",
    "            neg_combined = torch.stack(neg_combined, dim=1)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = contrastive_loss(anchor_combined, positive_combined, neg_combined)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2095ef0d",
   "metadata": {},
   "source": [
    "# Semantic Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6059e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_semantic_similarity(emb1, emb2, metric='cosine'):\n",
    "  \"\"\"\n",
    "  Calculate semantic similarity between two embeddings.\n",
    "  \n",
    "  Args:\n",
    "      emb1 (numpy.ndarray): First embedding vector\n",
    "      emb2 (numpy.ndarray): Second embedding vector\n",
    "      metric (str): Similarity metric to use. Options: 'cosine', 'euclidean', 'dot'\n",
    "  \n",
    "  Returns:\n",
    "      float: Similarity score between the embeddings\n",
    "  \"\"\"\n",
    "  # Convert to numpy arrays if they aren't already\n",
    "  emb1 = np.array(emb1)\n",
    "  emb2 = np.array(emb2)\n",
    "  \n",
    "  # Ensure vectors have the same shape\n",
    "  assert emb1.shape == emb2.shape, f\"Embeddings must have same shape. Got {emb1.shape} and {emb2.shape}\"\n",
    "  \n",
    "  if metric == 'cosine':\n",
    "      # Compute cosine similarity\n",
    "      norm1 = np.linalg.norm(emb1)\n",
    "      norm2 = np.linalg.norm(emb2)\n",
    "      if norm1 == 0 or norm2 == 0:\n",
    "          return 0.0\n",
    "      return np.dot(emb1, emb2) / (norm1 * norm2)\n",
    "  \n",
    "  elif metric == 'euclidean':\n",
    "      # Compute negative euclidean distance (converted to similarity)\n",
    "      # We use negative distance so that smaller distances = higher similarity\n",
    "      return -np.linalg.norm(emb1 - emb2)\n",
    "  \n",
    "  elif metric == 'dot':\n",
    "      # Simple dot product\n",
    "      return np.dot(emb1, emb2)\n",
    "  \n",
    "  else:\n",
    "      raise ValueError(f\"Unknown metric: {metric}. Choose from 'cosine', 'euclidean', or 'dot'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05757612",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d322e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Question: Given clustered user representations, can we map users in the same cluster\n",
    "          to the organization org chart?\n",
    "Approach: 1) Perform clustering on user representations - matching topic/meta embeddings\n",
    "          2) Compare the user_ids in these clusters to actual org chart, what's the match percentage\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral clustering for user embeddings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "# Get all user embeddings\n",
    "users_reps = merged_df[merged_df['TOPIC_is_user'] == True].copy()\n",
    "\n",
    "# Separate each into meta/topic features (topic feature columns start with TOPIC, everything else is META)\n",
    "id_columns = [\"META_is_user\", \"META_object_id\", \"TOPIC_is_user\", \"TOPIC_object_id\"]\n",
    "topic_columns = [col for col in users_reps.columns if col.startswith('TOPIC_') and col not in id_columns]\n",
    "meta_columns = [col for col in users_reps.columns if not col.startswith('TOPIC_') and col not in id_columns]\n",
    "\n",
    "# Process topic features (already numerical)\n",
    "topic_features = users_reps[topic_columns].copy()\n",
    "\n",
    "# Process meta features with categorical encoding\n",
    "meta_features = users_reps[meta_columns].copy()\n",
    "\n",
    "def compute_similarity_matrix(topic_features, meta_features, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Compute similarity matrix between users using both topic and meta features\n",
    "    alpha: weight for topic similarity (1-alpha will be weight for meta similarity)\n",
    "    \"\"\"\n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    topic_features_norm = scaler.fit_transform(topic_features)\n",
    "    meta_features_norm = scaler.fit_transform(meta_features)\n",
    "    \n",
    "    # Compute similarities\n",
    "    topic_sim = cosine_similarity(topic_features_norm)\n",
    "    meta_sim = cosine_similarity(meta_features_norm)\n",
    "    \n",
    "    # Late fusion with weighted combination\n",
    "    total_sim = alpha * topic_sim + (1 - alpha) * meta_sim\n",
    "    return total_sim\n",
    "\n",
    "# Compute similarity matrix with encoded features\n",
    "similarity_matrix = compute_similarity_matrix(topic_features, meta_features, alpha=0.6)\n",
    "\n",
    "# Rescale [0,1]\n",
    "similarity_matrix = (similarity_matrix - similarity_matrix.min()) / (similarity_matrix.max() - similarity_matrix.min())\n",
    "\n",
    "# Perform spectral clustering\n",
    "n_clusters = 10  # Adjust based on expected org chart structure\n",
    "clustering = SpectralClustering(n_clusters=n_clusters, \n",
    "                              affinity='precomputed',\n",
    "                              random_state=42)\n",
    "cluster_labels = clustering.fit_predict(similarity_matrix)\n",
    "\n",
    "# Map users to clusters\n",
    "user_buckets = [[] for _ in range(max(cluster_labels)+1)]\n",
    "for user_id, bucket in zip(users_reps['TOPIC_object_id'], cluster_labels):\n",
    "    user_buckets[bucket].append(user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2612e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Elbow method for spectal clustering ===\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score\n",
    ")\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "def evaluate_k_elbow(similarity_matrix, k_values, random_state=42):\n",
    "    similarity_matrix = np.array(similarity_matrix)\n",
    "    distance_matrix = (1 - similarity_matrix) / 2  # handle cosine-like sims safely\n",
    "\n",
    "    silhouette_scores = []\n",
    "    calinski_scores = []\n",
    "    davies_scores = []\n",
    "\n",
    "    # Optional embedding for metrics that need features\n",
    "    embedding = SpectralEmbedding(n_components=10, affinity='precomputed', random_state=random_state)\n",
    "    X_embed = embedding.fit_transform(similarity_matrix)\n",
    "\n",
    "    for k in k_values:\n",
    "        clustering = SpectralClustering(\n",
    "            n_clusters=k,\n",
    "            affinity='precomputed',\n",
    "            random_state=random_state\n",
    "        )\n",
    "        labels = clustering.fit_predict(similarity_matrix)\n",
    "\n",
    "        # --- Silhouette (higher is better)\n",
    "        sil = silhouette_score(distance_matrix, labels, metric=\"precomputed\")\n",
    "        silhouette_scores.append(sil)\n",
    "\n",
    "        # --- Calinski-Harabasz (higher is better)\n",
    "        calinski_scores.append(calinski_harabasz_score(X_embed, labels))\n",
    "\n",
    "        # --- Davies-Bouldin (lower is better)\n",
    "        davies_scores.append(davies_bouldin_score(X_embed, labels))\n",
    "\n",
    "        print(f\"k={k:2d} | Silhouette={sil:.3f} | CH={calinski_scores[-1]:.3f} | DB={davies_scores[-1]:.3f}\")\n",
    "\n",
    "    # --- Plot results ---\n",
    "    fig, ax1 = plt.subplots(figsize=(8,5))\n",
    "    ax1.plot(k_values, silhouette_scores, 'o-', label='Silhouette', color='C0')\n",
    "    ax1.plot(k_values, calinski_scores, 'o-', label='Calinski-Harabasz', color='C1')\n",
    "    ax1.set_xlabel(\"Number of clusters (k)\")\n",
    "    ax1.set_ylabel(\"Score\")\n",
    "    ax1.legend(loc='best')\n",
    "    ax1.grid(True)\n",
    "    plt.title(\"Elbow Analysis for Spectral Clustering\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(k_values, davies_scores, 'o-', color='C2', label='Davies-Bouldin (lower is better)')\n",
    "    plt.xlabel(\"Number of clusters (k)\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    print(\"✅ Done! Look for the elbow / peak in the plots.\")\n",
    "\n",
    "k_values = range(2, 20)\n",
    "evaluate_k_elbow(similarity_matrix, k_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3a4558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clustering Quality Visualization Toolkit ===\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    silhouette_score,\n",
    "    silhouette_samples,\n",
    "    calinski_harabasz_score,\n",
    "    davies_bouldin_score\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "\n",
    "def evaluate_clustering_from_similarity(similarity_matrix, labels, random_state=42):\n",
    "    \"\"\"\n",
    "    Evaluate clustering quality given a precomputed similarity matrix.\n",
    "    Includes silhouette analysis (on distance form), cluster distribution,\n",
    "    and 2D embedding visualization (using spectral embedding).\n",
    "    \"\"\"\n",
    "    similarity_matrix = np.array(similarity_matrix)\n",
    "    n_clusters = len(np.unique(labels))\n",
    "    if n_clusters < 2:\n",
    "        print(\"Need at least 2 clusters for evaluation.\")\n",
    "        return\n",
    "\n",
    "    # --- Convert to distance matrix ---\n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "    distance_matrix = np.clip(distance_matrix, 0, None)\n",
    "\n",
    "    # --- Compute metrics ---\n",
    "    print(f\"Number of clusters: {n_clusters}\")\n",
    "    print(\"----- Cluster Metrics -----\")\n",
    "    try:\n",
    "        sil = silhouette_score(distance_matrix, labels, metric=\"precomputed\")\n",
    "        print(f\"Silhouette Score:        {sil:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Silhouette Score:        (failed: {e})\")\n",
    "\n",
    "    # Calinski-Harabasz and Davies-Bouldin require feature-like space, so we embed\n",
    "    embed = SpectralEmbedding(n_components=10, affinity='precomputed', random_state=random_state)\n",
    "    X_embed = embed.fit_transform(similarity_matrix)\n",
    "\n",
    "    print(f\"Calinski-Harabasz Score: {calinski_harabasz_score(X_embed, labels):.3f}\")\n",
    "    print(f\"Davies-Bouldin Score:    {davies_bouldin_score(X_embed, labels):.3f}\")\n",
    "\n",
    "    # --- Cluster size distribution ---\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    sns.barplot(x=unique, y=counts, palette=\"viridis\")\n",
    "    plt.title(\"Cluster Size Distribution\")\n",
    "    plt.xlabel(\"Cluster Label\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "    # --- Silhouette plot ---\n",
    "    try:\n",
    "        sil_samples = silhouette_samples(distance_matrix, labels, metric=\"precomputed\")\n",
    "        y_lower = 10\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        for i in range(n_clusters):\n",
    "            ith_vals = sil_samples[labels == i]\n",
    "            ith_vals.sort()\n",
    "            size_i = ith_vals.shape[0]\n",
    "            y_upper = y_lower + size_i\n",
    "            plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_vals, alpha=0.7)\n",
    "            plt.text(-0.05, y_lower + 0.5 * size_i, str(i))\n",
    "            y_lower = y_upper + 10\n",
    "        plt.axvline(x=np.mean(sil_samples), color=\"red\", linestyle=\"--\")\n",
    "        plt.title(\"Silhouette Plot per Cluster (from similarity matrix)\")\n",
    "        plt.xlabel(\"Silhouette Coefficient Values\")\n",
    "        plt.ylabel(\"Cluster\")\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Silhouette plot skipped: {e}\")\n",
    "\n",
    "    # --- 2D Visualization (Spectral Embedding) ---\n",
    "    from sklearn.decomposition import PCA\n",
    "    X_2d = PCA(n_components=2, random_state=random_state).fit_transform(X_embed)\n",
    "\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.scatterplot(x=X_2d[:,0], y=X_2d[:,1], hue=labels, palette=\"tab10\", s=40, alpha=0.8)\n",
    "    plt.title(\"Cluster Visualization (2D Spectral Embedding)\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend(title=\"Cluster\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"✅ Visualization complete.\")\n",
    "\n",
    "# ---- Example usage ----\n",
    "evaluate_clustering_from_similarity(similarity_matrix, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658ed86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Question: Given user topic/meta embeddings and file topic/meta embeddings, how well\n",
    "          does cosine similarity reflect actual file access patterns?\n",
    "Approach: 1) Get cosine similarity of every user with every file\n",
    "          2) Test with different thresholds, where above threshold means we label as user accessed file and below labeled as no \n",
    "          3) Get confusion matrix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053dbe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Question: Given x month of unobserved data for an organization randomly mixed with fake data,\n",
    "          can we predict which user/file accesses actually happened?\n",
    "Approach: 1) Get snapshot of data as of x months ago, create file/user embeddings at that time\n",
    "          2) For each event starting from x months ago, fetch the file/user embedding (real data)\n",
    "          3) Create fake events (n:1? what should the ratio be? Needs to be researched) by mismatching users and file historys\n",
    "          4) Using cosine similarity of user/file make an assessment, play around with thresholds, get the performance\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
